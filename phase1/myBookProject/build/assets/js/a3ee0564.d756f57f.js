"use strict";(self.webpackChunkmybookproject=self.webpackChunkmybookproject||[]).push([[5574],{1616(e,n,t){t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>m,frontMatter:()=>i,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"module-4/autonomous-humanoid","title":"Module 4, Chapter 3: Capstone - The Autonomous Humanoid","description":"Build complete end-to-end VLA system with voice command to action execution, coordinated with ROS 2 for humanoid robot autonomy","source":"@site/docs/module-4/03-autonomous-humanoid.md","sourceDirName":"module-4","slug":"/module-4/autonomous-humanoid","permalink":"/docs/module-4/autonomous-humanoid","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3,"title":"Module 4, Chapter 3: Capstone - The Autonomous Humanoid","description":"Build complete end-to-end VLA system with voice command to action execution, coordinated with ROS 2 for humanoid robot autonomy"},"sidebar":"tutorialSidebar","previous":{"title":"Module 4, Chapter 2: Cognitive Planning with Large Language Models","permalink":"/docs/module-4/llm-cognitive-planning"},"next":{"title":"Glossary","permalink":"/docs/glossary"}}');var a=t(4848),o=t(8453);const i={sidebar_position:3,title:"Module 4, Chapter 3: Capstone - The Autonomous Humanoid",description:"Build complete end-to-end VLA system with voice command to action execution, coordinated with ROS 2 for humanoid robot autonomy"},r="Module 4, Chapter 3: Capstone - The Autonomous Humanoid",l={},c=[{value:"Overview",id:"overview",level:2},{value:"Complete System Architecture",id:"complete-system-architecture",level:2},{value:"VLA System Components",id:"vla-system-components",level:3},{value:"System Integration Patterns",id:"system-integration-patterns",level:3},{value:"Voice Command Processing Pipeline",id:"voice-command-processing-pipeline",level:3},{value:"End-to-End Voice Command to Action Execution",id:"end-to-end-voice-command-to-action-execution",level:2},{value:"Voice Command Processing Flow",id:"voice-command-processing-flow",level:3},{value:"Coordinating VLA Components with ROS 2",id:"coordinating-vla-components-with-ros-2",level:2},{value:"ROS 2 Message Passing Patterns",id:"ros-2-message-passing-patterns",level:3},{value:"State Management and Coordination",id:"state-management-and-coordination",level:3},{value:"System Limitations and Future Extensions",id:"system-limitations-and-future-extensions",level:2},{value:"Current System Limitations",id:"current-system-limitations",level:3},{value:"Addressing Limitations",id:"addressing-limitations",level:3},{value:"Future Extensions",id:"future-extensions",level:3},{value:"Practical Exercise: Complete VLA System Implementation",id:"practical-exercise-complete-vla-system-implementation",level:2},{value:"Summary",id:"summary",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"module-4-chapter-3-capstone---the-autonomous-humanoid",children:"Module 4, Chapter 3: Capstone - The Autonomous Humanoid"})}),"\n",(0,a.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,a.jsx)(n.p,{children:"This capstone chapter brings together all the components from the previous chapters to create a complete Voice-Language-Action (VLA) system for humanoid robots. We'll build an end-to-end system that accepts voice commands, processes them through large language models for cognitive planning, and executes actions using ROS 2 control systems. This chapter demonstrates how to coordinate all VLA components with ROS 2 for a fully autonomous humanoid robot."}),"\n",(0,a.jsx)(n.h2,{id:"complete-system-architecture",children:"Complete System Architecture"}),"\n",(0,a.jsx)(n.h3,{id:"vla-system-components",children:"VLA System Components"}),"\n",(0,a.jsx)(n.p,{children:"The complete VLA system consists of four main interconnected components:"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Voice Recognition Layer"}),": Converts spoken commands to text using OpenAI Whisper"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Language Model Interface"}),": Processes natural language and generates action plans"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Action Execution Layer"}),": Translates plans to ROS 2 commands for robot control"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Coordination Layer"}),": Orchestrates the complete voice-to-action pipeline"]}),"\n"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-mermaid",children:"graph TB\n    A[Voice Command] --\x3e B[Whisper Speech Recognition]\n    B --\x3e C[Natural Language Processing]\n    C --\x3e D[LLM Cognitive Planning]\n    D --\x3e E[Action Plan Validation]\n    E --\x3e F[ROS 2 Action Mapping]\n    F --\x3e G[Robot Execution]\n    G --\x3e H[Feedback Loop]\n    H --\x3e C\n\n    I[Sensor Data] --\x3e J[Perception Pipeline]\n    J --\x3e K[State Estimation]\n    K --\x3e D\n    K --\x3e F\n\n    L[Environment Map] --\x3e D\n    L --\x3e F\n"})}),"\n",(0,a.jsx)(n.h3,{id:"system-integration-patterns",children:"System Integration Patterns"}),"\n",(0,a.jsx)(n.p,{children:"The VLA system follows a modular architecture pattern:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# Example: Complete VLA system architecture\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom sensor_msgs.msg import Image, LaserScan\nfrom geometry_msgs.msg import Twist, PoseStamped\nfrom tf2_ros import TransformBuffer, TransformListener\nimport threading\nimport queue\nimport time\nfrom dataclasses import dataclass\nfrom typing import Dict, Any, Optional\n\n@dataclass\nclass VLACommand:\n    """Structured representation of VLA command"""\n    id: str\n    timestamp: float\n    voice_text: str\n    parsed_intent: Dict[str, Any]\n    action_plan: Optional[Dict] = None\n    status: str = "pending"  # pending, processing, executing, completed, failed\n\nclass VLASystemCoordinator(Node):\n    def __init__(self):\n        super().__init__(\'vla_system_coordinator\')\n\n        # Initialize VLA components\n        self.voice_recognizer = VoiceRecognizer(self)\n        self.language_processor = LanguageModelProcessor(self)\n        self.action_planner = ActionPlanner(self)\n        self.executor = ActionExecutor(self)\n        self.safety_manager = SafetyManager(self)\n\n        # Publishers and subscribers\n        self.voice_sub = self.create_subscription(String, \'/voice_commands\', self.voice_callback, 10)\n        self.status_pub = self.create_publisher(String, \'/vla_system_status\', 10)\n        self.feedback_pub = self.create_publisher(String, \'/vla_feedback\', 10)\n\n        # TF for state estimation\n        self.tf_buffer = TransformBuffer(self)\n        self.tf_listener = TransformListener(self.tf_buffer, self)\n\n        # Internal state\n        self.active_commands = {}\n        self.command_queue = queue.Queue()\n        self.system_state = {\n            \'battery_level\': 100.0,\n            \'current_pose\': None,\n            \'obstacles_detected\': [],\n            \'task_in_progress\': None\n        }\n\n        # Threading for parallel processing\n        self.processing_thread = threading.Thread(target=self.process_commands, daemon=True)\n        self.processing_thread.start()\n\n        self.get_logger().info("VLA System Coordinator initialized")\n\n    def voice_callback(self, msg: String):\n        """Handle incoming voice commands"""\n        try:\n            # Create command structure\n            command = VLACommand(\n                id=f"cmd_{int(time.time() * 1000)}",\n                timestamp=time.time(),\n                voice_text=msg.data,\n                parsed_intent={}\n            )\n\n            # Add to processing queue\n            self.command_queue.put(command)\n\n            # Update status\n            self.publish_status(f"Received voice command: {msg.data}")\n\n        except Exception as e:\n            self.get_logger().error(f"Error processing voice command: {e}")\n\n    def process_commands(self):\n        """Background thread to process commands from queue"""\n        while rclpy.ok():\n            try:\n                # Get command from queue\n                command = self.command_queue.get(timeout=0.1)\n\n                # Process command through VLA pipeline\n                self.active_commands[command.id] = command\n                self.execute_vla_pipeline(command)\n\n            except queue.Empty:\n                continue\n            except Exception as e:\n                self.get_logger().error(f"Error in command processing thread: {e}")\n\n    def execute_vla_pipeline(self, command: VLACommand):\n        """Execute complete VLA pipeline for a command"""\n        try:\n            self.get_logger().info(f"Processing VLA command: {command.voice_text}")\n\n            # Step 1: Parse natural language intent\n            self.publish_status("Parsing natural language intent...")\n            intent = self.language_processor.parse_intent(command.voice_text)\n            command.parsed_intent = intent\n\n            # Step 2: Check safety constraints\n            self.publish_status("Checking safety constraints...")\n            is_safe, safety_reason = self.safety_manager.validate_intent(intent)\n            if not is_safe:\n                self.get_logger().warn(f"Safety violation: {safety_reason}")\n                self.publish_feedback(f"Command rejected: {safety_reason}")\n                command.status = "failed"\n                return\n\n            # Step 3: Generate action plan\n            self.publish_status("Generating action plan...")\n            action_plan = self.action_planner.create_plan(intent, self.system_state)\n            command.action_plan = action_plan\n\n            # Step 4: Execute plan\n            self.publish_status("Executing action plan...")\n            command.status = "executing"\n            execution_result = self.executor.execute_plan(action_plan)\n\n            if execution_result.success:\n                command.status = "completed"\n                self.publish_feedback(f"Command completed: {command.voice_text}")\n                self.publish_status("Command completed successfully")\n            else:\n                command.status = "failed"\n                self.publish_feedback(f"Command failed: {execution_result.error}")\n                self.publish_status(f"Command failed: {execution_result.error}")\n\n        except Exception as e:\n            self.get_logger().error(f"Error in VLA pipeline: {e}")\n            command.status = "failed"\n            self.publish_feedback(f"System error: {str(e)}")\n\n    def publish_status(self, status: str):\n        """Publish system status"""\n        status_msg = String()\n        status_msg.data = status\n        self.status_pub.publish(status_msg)\n\n    def publish_feedback(self, feedback: str):\n        """Publish feedback to user"""\n        feedback_msg = String()\n        feedback_msg.data = feedback\n        self.feedback_pub.publish(feedback_msg)\n\n    def update_system_state(self):\n        """Update system state from various sensors"""\n        # Get robot pose\n        try:\n            transform = self.tf_buffer.lookup_transform(\n                \'map\', \'base_link\', rclpy.time.Time(), timeout=rclpy.duration.Duration(seconds=0.1)\n            )\n            self.system_state[\'current_pose\'] = transform.transform\n        except:\n            pass  # Ignore if transform not available\n\n        # Update other state information (battery, sensors, etc.)\n        # This would connect to actual robot state topics in a real implementation\n'})}),"\n",(0,a.jsx)(n.h3,{id:"voice-command-processing-pipeline",children:"Voice Command Processing Pipeline"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# Example: Voice command processing pipeline\nclass VoiceRecognizer:\n    def __init__(self, node):\n        self.node = node\n        self.whisper_model = self._load_whisper_model()\n\n    def _load_whisper_model(self):\n        """Load Whisper model for speech recognition"""\n        import whisper\n        # Use \'base\' or \'small\' for real-time performance\n        return whisper.load_model("base")\n\n    def recognize_speech(self, audio_data):\n        """Recognize speech from audio data"""\n        # Convert audio data to format suitable for Whisper\n        audio_array = self._convert_audio_format(audio_data)\n\n        # Transcribe audio\n        result = self.whisper_model.transcribe(audio_array)\n\n        return result["text"]\n\n    def _convert_audio_format(self, audio_data):\n        """Convert audio data to format expected by Whisper"""\n        # Implementation would convert audio format as needed\n        # For now, return the data as-is\n        return audio_data\n\nclass LanguageModelProcessor:\n    def __init__(self, node):\n        self.node = node\n        self.client = self._initialize_llm_client()\n\n    def _initialize_llm_client(self):\n        """Initialize LLM client"""\n        import openai\n        api_key = self.node.get_parameter_or(\'openai_api_key\', \'dummy_key\')\n        return openai.OpenAI(api_key=api_key)\n\n    def parse_intent(self, text: str) -> Dict[str, Any]:\n        """Parse intent from natural language text using LLM"""\n\n        # Create system prompt for robotic intent parsing\n        system_prompt = """\n        You are a robotic command interpreter. Parse the following natural language command\n        and extract structured intent information for a humanoid robot.\n\n        The output should be a JSON object with these fields:\n        {\n            "action": "move|navigate|manipulate|perceive|communicate|other",\n            "target": "specific object, location, or person",\n            "parameters": {\n                "distance": float (in meters),\n                "direction": "forward|backward|left|right|up|down",\n                "speed": float (in m/s),\n                "duration": float (in seconds)\n            },\n            "priority": "high|medium|low",\n            "urgency": "immediate|soon|normal"\n        }\n\n        Only return the JSON object, nothing else.\n        """\n\n        try:\n            response = self.client.chat.completions.create(\n                model="gpt-4",\n                messages=[\n                    {"role": "system", "content": system_prompt},\n                    {"role": "user", "content": text}\n                ],\n                temperature=0.1,\n                response_format={"type": "json_object"}\n            )\n\n            intent_data = json.loads(response.choices[0].message.content)\n            return intent_data\n\n        except Exception as e:\n            self.node.get_logger().error(f"Error parsing intent: {e}")\n            # Return default intent for error case\n            return {\n                "action": "unknown",\n                "target": None,\n                "parameters": {},\n                "priority": "low",\n                "urgency": "normal"\n            }\n\nclass ActionPlanner:\n    def __init__(self, node):\n        self.node = node\n\n    def create_plan(self, intent: Dict[str, Any], system_state: Dict[str, Any]) -> Dict[str, Any]:\n        """Create action plan based on intent and system state"""\n\n        # Map intent to executable actions\n        action_plan = {\n            "id": f"plan_{int(time.time() * 1000)}",\n            "intent": intent,\n            "steps": [],\n            "constraints": [],\n            "estimated_duration": 0.0\n        }\n\n        # Create plan based on action type\n        action_type = intent.get("action", "unknown")\n\n        if action_type == "move":\n            action_plan["steps"] = self._create_move_plan(intent)\n        elif action_type == "navigate":\n            action_plan["steps"] = self._create_navigation_plan(intent, system_state)\n        elif action_type == "manipulate":\n            action_plan["steps"] = self._create_manipulation_plan(intent)\n        elif action_type == "perceive":\n            action_plan["steps"] = self._create_perception_plan(intent)\n        else:\n            action_plan["steps"] = self._create_default_plan(intent)\n\n        # Add safety constraints\n        action_plan["constraints"] = self._add_safety_constraints(intent)\n\n        return action_plan\n\n    def _create_move_plan(self, intent: Dict[str, Any]) -> list:\n        """Create movement plan from intent"""\n        steps = []\n\n        # Extract movement parameters\n        direction = intent.get("parameters", {}).get("direction", "forward")\n        distance = intent.get("parameters", {}).get("distance", 1.0)\n        speed = intent.get("parameters", {}).get("speed", 0.3)\n\n        # Add movement step\n        steps.append({\n            "id": "move_step_1",\n            "action": "move_base",\n            "parameters": {\n                "direction": direction,\n                "distance": distance,\n                "speed": speed\n            },\n            "timeout": 30.0,\n            "recovery_plan": "stop_and_report"\n        })\n\n        return steps\n\n    def _create_navigation_plan(self, intent: Dict[str, Any], system_state: Dict[str, Any]) -> list:\n        """Create navigation plan from intent"""\n        steps = []\n\n        target = intent.get("target", "")\n        if not target:\n            self.node.get_logger().error("No target specified for navigation")\n            return []\n\n        # Add navigation steps\n        steps.append({\n            "id": "navigate_to_target",\n            "action": "move_base",\n            "parameters": {\n                "destination": target,\n                "speed": 0.3,\n                "avoid_obstacles": True\n            },\n            "timeout": 120.0,\n            "recovery_plan": "return_to_origin"\n        })\n\n        return steps\n\n    def _create_manipulation_plan(self, intent: Dict[str, Any]) -> list:\n        """Create manipulation plan from intent"""\n        steps = []\n\n        target = intent.get("target", "")\n        if not target:\n            self.node.get_logger().error("No target specified for manipulation")\n            return []\n\n        # Add perception step to locate object\n        steps.append({\n            "id": "locate_object",\n            "action": "perceive_object",\n            "parameters": {"target": target},\n            "timeout": 10.0,\n            "recovery_plan": "abort_manipulation"\n        })\n\n        # Add approach step\n        steps.append({\n            "id": "approach_object",\n            "action": "move_base",\n            "parameters": {\n                "distance": 0.5,\n                "speed": 0.1\n            },\n            "timeout": 30.0,\n            "recovery_plan": "retreat_and_retry"\n        })\n\n        # Add manipulation step\n        steps.append({\n            "id": "manipulate_object",\n            "action": "manipulator_control",\n            "parameters": {\n                "target": target,\n                "operation": "grasp"\n            },\n            "timeout": 30.0,\n            "recovery_plan": "release_and_abort"\n        })\n\n        return steps\n\n    def _add_safety_constraints(self, intent: Dict[str, Any]) -> list:\n        """Add safety constraints based on intent"""\n        constraints = []\n\n        # Add constraints based on action type\n        action = intent.get("action", "")\n        if action in ["navigate", "move"]:\n            constraints.append("maintain_safe_distance_from_obstacles")\n            constraints.append("validate_navigation_path_feasibility")\n\n        if action == "manipulate":\n            constraints.append("validate_object_graspability")\n            constraints.append("check_manipulation_safety_zone")\n\n        # Add priority-based constraints\n        priority = intent.get("priority", "normal")\n        if priority == "high":\n            constraints.append("reduce_safety_margins")\n        elif priority == "low":\n            constraints.append("increase_safety_margins")\n\n        return constraints\n'})}),"\n",(0,a.jsx)(n.h2,{id:"end-to-end-voice-command-to-action-execution",children:"End-to-End Voice Command to Action Execution"}),"\n",(0,a.jsx)(n.h3,{id:"voice-command-processing-flow",children:"Voice Command Processing Flow"}),"\n",(0,a.jsx)(n.p,{children:"The complete flow from voice command to action execution involves several stages:"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Audio Capture"}),": Microphone captures voice command"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Speech Recognition"}),": Whisper converts audio to text"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Intent Parsing"}),": LLM extracts structured intent"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Plan Generation"}),": Action planner creates executable plan"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Safety Validation"}),": Safety manager validates plan"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"ROS 2 Execution"}),": Plan executed through ROS 2 interfaces"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Feedback Loop"}),": Results reported back to user"]}),"\n"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# Example: Complete voice-to-action execution\nclass VoiceToActionExecutor:\n    def __init__(self, node):\n        self.node = node\n        self.coordinator = VLASystemCoordinator(node)\n\n    def execute_voice_command(self, voice_text: str) -> bool:\n        """Execute complete voice command to action flow"""\n\n        self.node.get_logger().info(f"Executing voice command: {voice_text}")\n\n        try:\n            # Step 1: Parse intent from voice text\n            intent = self.coordinator.language_processor.parse_intent(voice_text)\n\n            # Step 2: Validate safety\n            is_safe, reason = self.coordinator.safety_manager.validate_intent(intent)\n            if not is_safe:\n                self.node.get_logger().warn(f"Command blocked by safety: {reason}")\n                self.coordinator.publish_feedback(f"Command blocked: {reason}")\n                return False\n\n            # Step 3: Create action plan\n            system_state = self.coordinator.system_state\n            action_plan = self.coordinator.action_planner.create_plan(intent, system_state)\n\n            # Step 4: Execute plan with feedback\n            self.coordinator.publish_status("Executing action plan...")\n            result = self.coordinator.executor.execute_plan(action_plan)\n\n            if result.success:\n                self.coordinator.publish_status("Command completed successfully")\n                self.coordinator.publish_feedback("Command completed successfully")\n                return True\n            else:\n                self.coordinator.publish_status(f"Command failed: {result.error}")\n                self.coordinator.publish_feedback(f"Command failed: {result.error}")\n                return False\n\n        except Exception as e:\n            self.node.get_logger().error(f"Error in voice-to-action execution: {e}")\n            self.coordinator.publish_feedback(f"System error: {str(e)}")\n            return False\n\nclass ActionExecutor:\n    def __init__(self, node):\n        self.node = node\n\n        # ROS 2 interfaces for robot control\n        self.cmd_vel_pub = self.node.create_publisher(Twist, \'/cmd_vel\', 10)\n        self.nav_goal_pub = self.node.create_publisher(PoseStamped, \'/move_base_simple/goal\', 10)\n\n        # Action clients for more complex tasks\n        self.nav_client = ActionClient(self.node, NavigateToPose, \'navigate_to_pose\')\n\n    def execute_plan(self, plan: Dict[str, Any]):\n        """Execute action plan"""\n\n        result = ExecutionResult(success=True, error="", details=[])\n\n        for step in plan.get(\'steps\', []):\n            step_result = self.execute_step(step)\n\n            if not step_result.success:\n                result.success = False\n                result.error = step_result.error\n                result.details.append(f"Step {step[\'id\']} failed: {step_result.error}")\n\n                # Execute recovery plan if available\n                recovery_plan = step.get(\'recovery_plan\', \'stop\')\n                recovery_result = self.execute_recovery(recovery_plan)\n\n                if not recovery_result.success:\n                    result.error += f"; Recovery also failed: {recovery_result.error}"\n                    break  # Stop execution if recovery fails\n            else:\n                result.details.append(f"Step {step[\'id\']} completed successfully")\n\n        return result\n\n    def execute_step(self, step: Dict[str, Any]) -> ExecutionResult:\n        """Execute individual action step"""\n\n        action_type = step.get(\'action\', \'unknown\')\n        parameters = step.get(\'parameters\', {})\n\n        try:\n            if action_type == \'move_base\':\n                return self._execute_move_base_step(parameters)\n            elif action_type == \'perceive_object\':\n                return self._execute_perceive_step(parameters)\n            elif action_type == \'manipulator_control\':\n                return self._execute_manipulation_step(parameters)\n            elif action_type == \'speak\':\n                return self._execute_speak_step(parameters)\n            else:\n                return ExecutionResult(\n                    success=False,\n                    error=f"Unknown action type: {action_type}",\n                    details=[]\n                )\n\n        except Exception as e:\n            return ExecutionResult(\n                success=False,\n                error=str(e),\n                details=[]\n            )\n\n    def _execute_move_base_step(self, params: Dict[str, Any]) -> ExecutionResult:\n        """Execute movement step"""\n\n        try:\n            # For simple movement, publish velocity commands\n            if \'direction\' in params and \'distance\' in params:\n                direction = params[\'direction\']\n                distance = params[\'distance\']\n                speed = params.get(\'speed\', 0.3)\n\n                # Calculate movement time\n                duration = distance / speed\n\n                # Create velocity command\n                cmd_vel = Twist()\n\n                if direction == \'forward\':\n                    cmd_vel.linear.x = speed\n                elif direction == \'backward\':\n                    cmd_vel.linear.x = -speed\n                elif direction == \'left\':\n                    cmd_vel.linear.y = speed\n                elif direction == \'right\':\n                    cmd_vel.linear.y = -speed\n                elif direction == \'rotate_left\':\n                    cmd_vel.angular.z = speed\n                elif direction == \'rotate_right\':\n                    cmd_vel.angular.z = -speed\n\n                # Execute movement\n                start_time = self.node.get_clock().now()\n                end_time = start_time + rclpy.duration.Duration(seconds=duration)\n\n                while self.node.get_clock().now() < end_time and rclpy.ok():\n                    self.cmd_vel_pub.publish(cmd_vel)\n                    time.sleep(0.1)  # 10Hz control\n\n                # Stop robot\n                stop_cmd = Twist()\n                self.cmd_vel_pub.publish(stop_cmd)\n\n                return ExecutionResult(success=True, error="", details=["Movement completed"])\n\n            else:\n                # For navigation to specific location\n                if \'destination\' in params:\n                    return self._execute_navigation_to_location(params[\'destination\'])\n\n                return ExecutionResult(\n                    success=False,\n                    error="Neither direction/distance nor destination specified",\n                    details=[]\n                )\n\n        except Exception as e:\n            return ExecutionResult(\n                success=False,\n                error=f"Movement execution failed: {str(e)}",\n                details=[]\n            )\n\n    def _execute_navigation_to_location(self, destination: str) -> ExecutionResult:\n        """Execute navigation to specific location"""\n\n        try:\n            # In a real implementation, this would look up the destination\n            # in a semantic map or use navigation goals\n            pose = self._lookup_destination_pose(destination)\n\n            if pose is None:\n                return ExecutionResult(\n                    success=False,\n                    error=f"Destination not found: {destination}",\n                    details=[]\n                )\n\n            # Publish navigation goal\n            self.nav_goal_pub.publish(pose)\n\n            return ExecutionResult(\n                success=True,\n                error="",\n                details=[f"Navigating to {destination}"]\n            )\n\n        except Exception as e:\n            return ExecutionResult(\n                success=False,\n                error=f"Navigation failed: {str(e)}",\n                details=[]\n            )\n\n    def _lookup_destination_pose(self, destination: str) -> Optional[PoseStamped]:\n        """Lookup destination pose in semantic map"""\n        # This would connect to a semantic map service in a real implementation\n        # For now, return a dummy pose based on destination name\n        pose = PoseStamped()\n        pose.header.frame_id = \'map\'\n        pose.header.stamp = self.node.get_clock().now().to_msg()\n\n        # Map destinations to approximate coordinates\n        if destination.lower() in [\'kitchen\', \'kitchen_area\']:\n            pose.pose.position.x = 3.0\n            pose.pose.position.y = 2.0\n        elif destination.lower() in [\'living_room\', \'living area\']:\n            pose.pose.position.x = 1.0\n            pose.pose.position.y = 1.0\n        elif destination.lower() in [\'bedroom\', \'bedroom_area\']:\n            pose.pose.position.x = 5.0\n            pose.pose.position.y = 1.0\n        else:\n            # Default to current position + offset\n            pose.pose.position.x = 2.0\n            pose.pose.position.y = 0.0\n\n        pose.pose.orientation.w = 1.0  # No rotation\n\n        return pose\n\n    def execute_recovery(self, recovery_type: str) -> ExecutionResult:\n        """Execute recovery action"""\n\n        try:\n            if recovery_type == \'stop_and_report\':\n                # Stop robot and report error\n                stop_cmd = Twist()\n                self.cmd_vel_pub.publish(stop_cmd)\n                return ExecutionResult(success=True, error="", details=["Stopped robot"])\n\n            elif recovery_type == \'return_to_origin\':\n                # Navigate back to origin\n                origin = PoseStamped()\n                origin.header.frame_id = \'map\'\n                origin.header.stamp = self.node.get_clock().now().to_msg()\n                origin.pose.position.x = 0.0\n                origin.pose.position.y = 0.0\n                origin.pose.orientation.w = 1.0\n\n                self.nav_goal_pub.publish(origin)\n                return ExecutionResult(success=True, error="", details=["Returning to origin"])\n\n            elif recovery_type == \'abort_and_retry\':\n                # Simply report that we\'re aborting\n                return ExecutionResult(success=True, error="", details=["Aborting and ready to retry"])\n\n            else:\n                return ExecutionResult(\n                    success=False,\n                    error=f"Unknown recovery type: {recovery_type}",\n                    details=[]\n                )\n\n        except Exception as e:\n            return ExecutionResult(\n                success=False,\n                error=f"Recovery execution failed: {str(e)}",\n                details=[]\n            )\n\n@dataclass\nclass ExecutionResult:\n    """Result of action execution"""\n    success: bool\n    error: str\n    details: list\n'})}),"\n",(0,a.jsx)(n.h2,{id:"coordinating-vla-components-with-ros-2",children:"Coordinating VLA Components with ROS 2"}),"\n",(0,a.jsx)(n.h3,{id:"ros-2-message-passing-patterns",children:"ROS 2 Message Passing Patterns"}),"\n",(0,a.jsx)(n.p,{children:"The VLA system uses various ROS 2 message passing patterns to coordinate components:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# Example: ROS 2 coordination patterns for VLA system\nimport rclpy\nfrom rclpy.qos import QoSProfile, DurabilityPolicy\nfrom std_msgs.msg import String, Bool\nfrom sensor_msgs.msg import Image, LaserScan\nfrom geometry_msgs.msg import Twist, PoseStamped\nfrom action_msgs.msg import GoalStatus\nfrom rclpy.action import ActionClient, ActionServer\n\nclass VLAComponentCoordinator:\n    def __init__(self, node):\n        self.node = node\n\n        # QoS profiles for different communication patterns\n        self.reliable_qos = QoSProfile(\n            depth=10,\n            durability=DurabilityPolicy.TRANSIENT_LOCAL\n        )\n\n        self.best_effort_qos = QoSProfile(\n            depth=5,\n            durability=DurabilityPolicy.VOLATILE\n        )\n\n        # Publishers for component coordination\n        self.voice_command_pub = self.node.create_publisher(String, \'/vla/voice_command\', self.reliable_qos)\n        self.intent_pub = self.node.create_publisher(String, \'/vla/parsed_intent\', self.reliable_qos)\n        self.action_plan_pub = self.node.create_publisher(String, \'/vla/action_plan\', self.reliable_qos)\n        self.execution_status_pub = self.node.create_publisher(String, \'/vla/execution_status\', self.reliable_qos)\n\n        # Subscribers for coordination feedback\n        self.voice_sub = self.node.create_subscription(\n            String, \'/voice_commands\', self.voice_command_handler, self.reliable_qos\n        )\n        self.intent_sub = self.node.create_subscription(\n            String, \'/vla/parsed_intent\', self.intent_handler, self.reliable_qos\n        )\n        self.plan_sub = self.node.create_subscription(\n            String, \'/vla/action_plan\', self.plan_handler, self.reliable_qos\n        )\n        self.status_sub = self.node.create_subscription(\n            String, \'/vla/execution_status\', self.status_handler, self.best_effort_qos\n        )\n\n        # Service servers for component coordination\n        self.validate_intent_srv = self.node.create_service(\n            String, \'/vla/validate_intent\', self.validate_intent_callback\n        )\n        self.generate_plan_srv = self.node.create_service(\n            String, \'/vla/generate_plan\', self.generate_plan_callback\n        )\n\n        # Action servers for complex coordinated tasks\n        self.vla_execution_action = ActionServer(\n            self.node,\n            VLAExecution,\n            \'vla_execute\',\n            self.execute_vla_goal\n        )\n\n        self.get_logger().info("VLA Component Coordinator initialized")\n\n    def voice_command_handler(self, msg: String):\n        """Handle incoming voice commands and trigger processing pipeline"""\n\n        self.get_logger().info(f"Received voice command: {msg.data}")\n\n        # Trigger intent parsing\n        self.trigger_intent_parsing(msg.data)\n\n    def trigger_intent_parsing(self, voice_text: str):\n        """Trigger intent parsing in LLM component"""\n\n        # Publish to LLM processing component\n        intent_msg = String()\n        intent_msg.data = voice_text\n        self.voice_command_pub.publish(intent_msg)\n\n    def intent_handler(self, msg: String):\n        """Handle parsed intents and trigger planning"""\n\n        try:\n            intent_data = json.loads(msg.data)\n\n            self.get_logger().info(f"Received parsed intent: {intent_data.get(\'action\', \'unknown\')}")\n\n            # Trigger action planning\n            self.trigger_action_planning(intent_data)\n\n        except json.JSONDecodeError:\n            self.get_logger().error(f"Invalid JSON in intent message: {msg.data}")\n\n    def trigger_action_planning(self, intent_data: Dict[str, Any]):\n        """Trigger action planning for parsed intent"""\n\n        # Create planning request\n        plan_request = String()\n        plan_request.data = json.dumps(intent_data)\n\n        # Publish to planning component\n        self.intent_pub.publish(plan_request)\n\n    def plan_handler(self, msg: String):\n        """Handle generated action plans and trigger execution"""\n\n        try:\n            plan_data = json.loads(msg.data)\n\n            self.get_logger().info(f"Received action plan with {len(plan_data.get(\'steps\', []))} steps")\n\n            # Trigger plan execution\n            self.trigger_plan_execution(plan_data)\n\n        except json.JSONDecodeError:\n            self.get_logger().error(f"Invalid JSON in plan message: {msg.data}")\n\n    def trigger_plan_execution(self, plan_data: Dict[str, Any]):\n        """Trigger execution of action plan"""\n\n        # Execute plan through action executor\n        executor = ActionExecutor(self.node)\n        result = executor.execute_plan(plan_data)\n\n        # Publish execution status\n        status_msg = String()\n        status_msg.data = json.dumps({\n            \'success\': result.success,\n            \'error\': result.error,\n            \'details\': result.details\n        })\n        self.execution_status_pub.publish(status_msg)\n\n    def validate_intent_callback(self, request, response):\n        """Service callback to validate intent safety"""\n\n        try:\n            intent_data = json.loads(request.data)\n\n            safety_manager = SafetyManager(self.node)\n            is_safe, reason = safety_manager.validate_intent(intent_data)\n\n            response.data = json.dumps({\n                \'is_safe\': is_safe,\n                \'reason\': reason\n            })\n\n            return response\n\n        except Exception as e:\n            response.data = json.dumps({\n                \'is_safe\': False,\n                \'reason\': f"Validation error: {str(e)}"\n            })\n            return response\n\n    def generate_plan_callback(self, request, response):\n        """Service callback to generate action plan"""\n\n        try:\n            intent_data = json.loads(request.data)\n\n            planner = ActionPlanner(self.node)\n            system_state = self.get_system_state()\n            action_plan = planner.create_plan(intent_data, system_state)\n\n            response.data = json.dumps(action_plan)\n            return response\n\n        except Exception as e:\n            response.data = json.dumps({\n                \'error\': f"Plan generation error: {str(e)}"\n            })\n            return response\n\n    def execute_vla_goal(self, goal_handle):\n        """Execute VLA action goal with feedback"""\n\n        try:\n            self.get_logger().info(f"Executing VLA goal: {goal_handle.request.command}")\n\n            # Update goal status\n            goal_handle.publish_feedback(VLAExecution.Feedback(status="Processing voice command"))\n\n            # Parse voice command\n            language_processor = LanguageModelProcessor(self.node)\n            intent = language_processor.parse_intent(goal_handle.request.command)\n\n            goal_handle.publish_feedback(VLAExecution.Feedback(status="Validating safety constraints"))\n\n            # Validate safety\n            safety_manager = SafetyManager(self.node)\n            is_safe, reason = safety_manager.validate_intent(intent)\n            if not is_safe:\n                goal_handle.abort()\n                result = VLAExecution.Result()\n                result.success = False\n                result.error_message = f"Command blocked by safety: {reason}"\n                return result\n\n            goal_handle.publish_feedback(VLAExecution.Feedback(status="Generating action plan"))\n\n            # Generate plan\n            planner = ActionPlanner(self.node)\n            system_state = self.get_system_state()\n            action_plan = planner.create_plan(intent, system_state)\n\n            goal_handle.publish_feedback(VLAExecution.Feedback(status="Executing action plan"))\n\n            # Execute plan\n            executor = ActionExecutor(self.node)\n            execution_result = executor.execute_plan(action_plan)\n\n            if execution_result.success:\n                goal_handle.succeed()\n                result = VLAExecution.Result()\n                result.success = True\n                result.error_message = ""\n                result.details = execution_result.details\n            else:\n                goal_handle.abort()\n                result = VLAExecution.Result()\n                result.success = False\n                result.error_message = execution_result.error\n                result.details = execution_result.details\n\n            return result\n\n        except Exception as e:\n            self.get_logger().error(f"Error in VLA goal execution: {e}")\n            goal_handle.abort()\n            result = VLAExecution.Result()\n            result.success = False\n            result.error_message = f"System error: {str(e)}"\n            result.details = []\n            return result\n\n    def get_system_state(self) -> Dict[str, Any]:\n        """Get current system state for planning"""\n        # In a real implementation, this would gather state from various sources\n        return {\n            \'battery_level\': 100.0,\n            \'current_pose\': None,\n            \'obstacles_detected\': [],\n            \'task_in_progress\': None\n        }\n'})}),"\n",(0,a.jsx)(n.h3,{id:"state-management-and-coordination",children:"State Management and Coordination"}),"\n",(0,a.jsx)(n.p,{children:"Humanoid robots require careful state management for safe and coordinated operation:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# Example: State management for humanoid VLA system\nfrom enum import Enum\nimport threading\n\nclass RobotState(Enum):\n    IDLE = 1\n    LISTENING = 2\n    PROCESSING_VOICE = 3\n    PLANNING_ACTION = 4\n    EXECUTING_ACTION = 5\n    RECOVERING = 6\n    SAFETY_STOP = 7\n\nclass HumanoidStateManager:\n    def __init__(self, node):\n        self.node = node\n        self.state = RobotState.IDLE\n        self.state_lock = threading.Lock()\n\n        # State publishers\n        self.state_pub = self.node.create_publisher(String, \'/humanoid/state\', 10)\n        self.battery_pub = self.node.create_publisher(String, \'/humanoid/battery\', 10)\n\n        # State subscribers\n        self.emergency_stop_sub = self.node.create_subscription(\n            Bool, \'/emergency_stop\', self.emergency_stop_callback, 10\n        )\n\n        # Timers for state monitoring\n        self.state_monitor_timer = self.node.create_timer(0.1, self.state_monitor_callback)\n\n    def set_state(self, new_state: RobotState):\n        """Safely update robot state"""\n        with self.state_lock:\n            old_state = self.state\n            self.state = new_state\n\n            self.node.get_logger().info(f"State transition: {old_state.name} -> {self.state.name}")\n\n            # Publish state change\n            state_msg = String()\n            state_msg.data = self.state.name\n            self.state_pub.publish(state_msg)\n\n    def get_state(self) -> RobotState:\n        """Get current robot state"""\n        with self.state_lock:\n            return self.state\n\n    def can_accept_command(self) -> bool:\n        """Check if robot can accept new commands"""\n        with self.state_lock:\n            return self.state in [RobotState.IDLE, RobotState.LISTENING]\n\n    def state_monitor_callback(self):\n        """Monitor robot state and handle transitions"""\n        current_state = self.get_state()\n\n        # Check for safety conditions based on state\n        if current_state == RobotState.EXECUTING_ACTION:\n            # Monitor execution safety\n            if self.is_execution_unsafe():\n                self.set_state(RobotState.SAFETY_STOP)\n                self.execute_safety_procedures()\n        elif current_state == RobotState.SAFETY_STOP:\n            # Wait for manual reset or safe conditions\n            if self.are_safety_conditions_clear():\n                self.set_state(RobotState.IDLE)\n\n    def is_execution_unsafe(self) -> bool:\n        """Check if current execution is unsafe"""\n        # Check various safety conditions:\n        # - Battery level too low\n        # - Balance parameters outside safe range\n        # - Obstacles too close\n        # - Unexpected sensor readings\n\n        battery_level = self.get_battery_level()\n        if battery_level < 10:  # Less than 10% battery\n            self.node.get_logger().warn(f"Low battery: {battery_level}%")\n            return True\n\n        # Add other safety checks as needed\n        return False\n\n    def are_safety_conditions_clear(self) -> bool:\n        """Check if safety stop conditions are resolved"""\n        battery_level = self.get_battery_level()\n        return battery_level > 20  # Resume if battery above 20%\n\n    def emergency_stop_callback(self, msg: Bool):\n        """Handle emergency stop command"""\n        if msg.data:  # Emergency stop activated\n            self.set_state(RobotState.SAFETY_STOP)\n            self.execute_safety_procedures()\n        else:  # Emergency stop released\n            if self.are_safety_conditions_clear():\n                self.set_state(RobotState.IDLE)\n\n    def execute_safety_procedures(self):\n        """Execute safety procedures when stopping"""\n        # Stop all motion\n        cmd_vel = Twist()\n        cmd_vel.linear.x = 0.0\n        cmd_vel.linear.y = 0.0\n        cmd_vel.linear.z = 0.0\n        cmd_vel.angular.x = 0.0\n        cmd_vel.angular.y = 0.0\n        cmd_vel.angular.z = 0.0\n\n        # Publish stop command\n        cmd_vel_pub = self.node.create_publisher(Twist, \'/cmd_vel\', 10)\n        cmd_vel_pub.publish(cmd_vel)\n\n        # Log safety event\n        self.node.get_logger().warn("Safety procedures executed - robot stopped")\n\n    def get_battery_level(self) -> float:\n        """Get current battery level (placeholder implementation)"""\n        # In a real implementation, this would get battery data from robot\n        return 100.0  # Placeholder\n'})}),"\n",(0,a.jsx)(n.h2,{id:"system-limitations-and-future-extensions",children:"System Limitations and Future Extensions"}),"\n",(0,a.jsx)(n.h3,{id:"current-system-limitations",children:"Current System Limitations"}),"\n",(0,a.jsx)(n.p,{children:"The VLA system has several important limitations to consider:"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Computational Requirements"}),": LLM processing requires significant computational resources"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Latency"}),": Multi-stage processing introduces delays between command and execution"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Ambiguity Resolution"}),": Natural language can be ambiguous and difficult to interpret"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Context Awareness"}),": Limited ability to maintain complex context across interactions"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Safety Constraints"}),": LLM outputs need careful validation for safety-critical applications"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"addressing-limitations",children:"Addressing Limitations"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# Example: Handling system limitations\nclass VLAQualityManager:\n    def __init__(self, node):\n        self.node = node\n        self.response_times = []\n        self.success_rates = []\n        self.error_patterns = {}\n\n    def measure_response_time(self, start_time, end_time):\n        """Measure and log response time for quality assessment"""\n        response_time = end_time - start_time\n        self.response_times.append(response_time)\n\n        # Log slow responses\n        if response_time > 3.0:  # More than 3 seconds is slow\n            self.node.get_logger().warn(f"Slow response time: {response_time:.2f}s")\n\n    def track_execution_success(self, success: bool, command_type: str):\n        """Track execution success rates"""\n        self.success_rates.append((success, command_type))\n\n        if not success:\n            self.error_patterns[command_type] = self.error_patterns.get(command_type, 0) + 1\n\n    def get_performance_metrics(self) -> Dict[str, Any]:\n        """Get current performance metrics"""\n        if not self.response_times:\n            avg_response_time = 0.0\n        else:\n            avg_response_time = sum(self.response_times) / len(self.response_times)\n\n        if not self.success_rates:\n            success_rate = 0.0\n        else:\n            successful_executions = sum(1 for success, _ in self.success_rates if success)\n            success_rate = successful_executions / len(self.success_rates) * 100\n\n        return {\n            \'average_response_time\': avg_response_time,\n            \'success_rate\': success_rate,\n            \'total_executions\': len(self.success_rates),\n            \'error_distribution\': self.error_patterns\n        }\n\nclass VLASystemOptimizer:\n    def __init__(self, node):\n        self.node = node\n        self.quality_manager = VLAQualityManager(node)\n\n    def optimize_for_latency(self):\n        """Optimize system for reduced latency"""\n        # Use smaller, faster LLM models for simple commands\n        # Cache frequently used plans\n        # Optimize sensor data processing pipelines\n        pass\n\n    def handle_ambiguous_command(self, command_text: str) -> tuple[bool, str, str]:\n        """Handle potentially ambiguous commands"""\n\n        # Check if command is ambiguous\n        ambiguity_score = self.assess_command_ambiguity(command_text)\n\n        if ambiguity_score > 0.7:  # Highly ambiguous\n            # Ask for clarification\n            clarification_needed = self.generate_clarification_request(command_text)\n            return False, "Command is ambiguous", clarification_needed\n        elif ambiguity_score > 0.4:  # Moderately ambiguous\n            # Make best guess but inform user\n            best_guess = self.make_best_guess(command_text)\n            return True, f"Interpreted as: {best_guess}", best_guess\n        else:  # Clear command\n            return True, "Command is clear", command_text\n\n    def assess_command_ambiguity(self, command_text: str) -> float:\n        """Assess how ambiguous a command is (0.0 to 1.0)"""\n        # Simple heuristic - in a real system, this would use more sophisticated NLP\n        ambiguous_indicators = [\n            \'it\', \'that\', \'there\', \'thing\', \'stuff\',  # Vague references\n            \'?\',  # Questions are often ambiguous\n        ]\n\n        ambiguity_score = 0.0\n\n        for indicator in ambiguous_indicators:\n            if indicator in command_text.lower():\n                ambiguity_score += 0.2\n\n        # Penalize very short commands that might be incomplete\n        if len(command_text.split()) < 3:\n            ambiguity_score += 0.3\n\n        return min(ambiguity_score, 1.0)\n\n    def generate_clarification_request(self, command_text: str) -> str:\n        """Generate a clarification request for ambiguous command"""\n        return f"I\'m not sure what you mean by \'{command_text}\'. Could you please be more specific?"\n\n    def make_best_guess(self, command_text: str) -> str:\n        """Make best guess for moderately ambiguous command"""\n        # In a real implementation, this would use context and heuristics\n        return command_text\n'})}),"\n",(0,a.jsx)(n.h3,{id:"future-extensions",children:"Future Extensions"}),"\n",(0,a.jsx)(n.p,{children:"The system can be extended in several ways:"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Multimodal Integration"}),": Add vision-based command confirmation"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Learning Capabilities"}),": Adapt to user preferences and common commands"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Extended Safety"}),": More sophisticated safety validation and recovery"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Collaborative Robots"}),": Multi-robot coordination capabilities"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Long-term Memory"}),": Remember user preferences and context across sessions"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"practical-exercise-complete-vla-system-implementation",children:"Practical Exercise: Complete VLA System Implementation"}),"\n",(0,a.jsx)(n.p,{children:"Let's create a complete working example that demonstrates the full VLA system:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n# Complete VLA system implementation example\n\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String, Bool\nfrom geometry_msgs.msg import Twist\nfrom sensor_msgs.msg import LaserScan\nimport json\nimport threading\nimport time\nfrom dataclasses import dataclass\nfrom typing import Dict, Any, Optional\n\n@dataclass\nclass VLACommand:\n    id: str\n    timestamp: float\n    voice_text: str\n    parsed_intent: Optional[Dict[str, Any]] = None\n    action_plan: Optional[Dict[str, Any]] = None\n    status: str = "pending"\n\nclass CompleteVLASystem(Node):\n    def __init__(self):\n        super().__init__(\'complete_vla_system\')\n\n        # Initialize all components\n        self.voice_recognizer = VoiceRecognizer(self)\n        self.language_processor = LanguageModelProcessor(self)\n        self.action_planner = ActionPlanner(self)\n        self.executor = ActionExecutor(self)\n        self.safety_manager = SafetyManager(self)\n        self.state_manager = HumanoidStateManager(self)\n        self.quality_manager = VLAQualityManager(self)\n        self.optimizer = VLASystemOptimizer(self)\n\n        # Publishers and subscribers\n        self.voice_sub = self.create_subscription(String, \'/voice_commands\', self.voice_callback, 10)\n        self.status_pub = self.create_publisher(String, \'/vla_system_status\', 10)\n        self.cmd_vel_pub = self.create_publisher(Twist, \'/cmd_vel\', 10)\n        self.scan_sub = self.create_subscription(LaserScan, \'/scan\', self.scan_callback, 10)\n\n        # Internal state\n        self.active_commands = {}\n        self.command_queue = queue.Queue()\n        self.system_state = {\n            \'battery_level\': 100.0,\n            \'current_pose\': None,\n            \'obstacles_detected\': [],\n            \'task_in_progress\': None\n        }\n\n        # Processing thread\n        self.processing_thread = threading.Thread(target=self.process_commands, daemon=True)\n        self.processing_thread.start()\n\n        # Performance monitoring\n        self.performance_timer = self.create_timer(10.0, self.performance_report)\n\n        self.get_logger().info("Complete VLA System initialized and ready")\n\n    def voice_callback(self, msg: String):\n        """Handle incoming voice commands"""\n        try:\n            # Check if system can accept commands\n            if not self.state_manager.can_accept_command():\n                self.get_logger().warn("System busy, rejecting command")\n                return\n\n            # Assess command clarity\n            is_clear, message, clarified_command = self.optimizer.handle_ambiguous_command(msg.data)\n\n            if not is_clear:\n                self.publish_status(f"Ambiguous command: {message}")\n                # In a real system, you\'d ask the user for clarification\n                # For this demo, we\'ll reject ambiguous commands\n                return\n\n            # Create command structure\n            command = VLACommand(\n                id=f"cmd_{int(time.time() * 1000)}",\n                timestamp=time.time(),\n                voice_text=clarified_command\n            )\n\n            # Add to processing queue\n            self.command_queue.put(command)\n            self.active_commands[command.id] = command\n\n            # Update state\n            self.state_manager.set_state(RobotState.PROCESSING_VOICE)\n\n            self.get_logger().info(f"Queued command: {command.voice_text}")\n\n        except Exception as e:\n            self.get_logger().error(f"Error handling voice command: {e}")\n\n    def scan_callback(self, msg: LaserScan):\n        """Update obstacle information from laser scan"""\n        # Process scan to detect obstacles\n        obstacles = []\n        for i, range_val in enumerate(msg.ranges):\n            if 0.1 < range_val < 2.0:  # Valid range, within 2m\n                angle = msg.angle_min + i * msg.angle_increment\n                x = range_val * math.cos(angle)\n                y = range_val * math.sin(angle)\n                obstacles.append({\'x\': x, \'y\': y, \'distance\': range_val})\n\n        self.system_state[\'obstacles_detected\'] = obstacles\n\n    def process_commands(self):\n        """Process commands from queue in background thread"""\n        while rclpy.ok():\n            try:\n                command = self.command_queue.get(timeout=0.1)\n\n                start_time = time.time()\n\n                # Process command through VLA pipeline\n                success = self.execute_vla_pipeline(command)\n\n                end_time = time.time()\n\n                # Track performance\n                self.quality_manager.measure_response_time(start_time, end_time)\n                self.quality_manager.track_execution_success(success, command.parsed_intent.get(\'action\', \'unknown\') if command.parsed_intent else \'unknown\')\n\n                # Update state\n                if success:\n                    self.state_manager.set_state(RobotState.IDLE)\n                else:\n                    self.state_manager.set_state(RobotState.RECOVERING)\n\n            except queue.Empty:\n                continue\n            except Exception as e:\n                self.get_logger().error(f"Error in command processing thread: {e}")\n                self.state_manager.set_state(RobotState.RECOVERING)\n\n    def execute_vla_pipeline(self, command: VLACommand) -> bool:\n        """Execute complete VLA pipeline for command"""\n        try:\n            self.publish_status(f"Processing: {command.voice_text}")\n\n            # Step 1: Parse intent\n            self.publish_status("Parsing intent...")\n            intent = self.language_processor.parse_intent(command.voice_text)\n            command.parsed_intent = intent\n\n            # Step 2: Validate safety\n            self.publish_status("Checking safety...")\n            is_safe, reason = self.safety_manager.validate_intent(intent)\n            if not is_safe:\n                self.get_logger().warn(f"Command blocked by safety: {reason}")\n                self.publish_status(f"Blocked: {reason}")\n                return False\n\n            # Step 3: Create action plan\n            self.publish_status("Planning action...")\n            action_plan = self.action_planner.create_plan(intent, self.system_state)\n            command.action_plan = action_plan\n\n            # Step 4: Execute plan\n            self.publish_status("Executing plan...")\n            self.state_manager.set_state(RobotState.EXECUTING_ACTION)\n\n            execution_result = self.executor.execute_plan(action_plan)\n\n            if execution_result.success:\n                self.publish_status("Command completed successfully")\n                self.publish_feedback(f"Completed: {command.voice_text}")\n                return True\n            else:\n                self.publish_status(f"Execution failed: {execution_result.error}")\n                return False\n\n        except Exception as e:\n            self.get_logger().error(f"Error in VLA pipeline: {e}")\n            self.publish_status(f"System error: {str(e)}")\n            return False\n\n    def performance_report(self):\n        """Publish periodic performance report"""\n        metrics = self.quality_manager.get_performance_metrics()\n\n        report = f"Performance - Avg response: {metrics[\'average_response_time\']:.2f}s, " \\\n                 f"Success rate: {metrics[\'success_rate\']:.1f}%, " \\\n                 f"Total executions: {metrics[\'total_executions\']}"\n\n        self.get_logger().info(report)\n\n    def publish_status(self, status: str):\n        """Publish system status"""\n        status_msg = String()\n        status_msg.data = status\n        self.status_pub.publish(status_msg)\n\n    def publish_feedback(self, feedback: str):\n        """Publish feedback to user"""\n        feedback_msg = String()\n        feedback_msg.data = feedback\n        # In a real system, this might go to a speech synthesis node or display\n        pass\n\ndef main(args=None):\n    rclpy.init(args=args)\n\n    vla_system = CompleteVLASystem()\n\n    try:\n        rclpy.spin(vla_system)\n    except KeyboardInterrupt:\n        vla_system.get_logger().info("Shutting down VLA system...")\n    finally:\n        vla_system.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,a.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,a.jsx)(n.p,{children:"In this capstone chapter, you've learned how to:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Build a complete end-to-end VLA system that coordinates voice recognition, language processing, and action execution"}),"\n",(0,a.jsx)(n.li,{children:"Implement proper ROS 2 message passing patterns for component coordination"}),"\n",(0,a.jsx)(n.li,{children:"Manage robot state and safety during complex VLA operations"}),"\n",(0,a.jsx)(n.li,{children:"Address system limitations and plan for future extensions"}),"\n",(0,a.jsx)(n.li,{children:"Create a complete working system that processes voice commands into robot actions"}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"The complete VLA system provides a foundation for autonomous humanoid robots that can understand natural language commands and execute them safely in real-world environments. This system integrates all the components from previous chapters into a cohesive whole that demonstrates the power of combining voice recognition, large language models, and ROS 2 for advanced humanoid robotics applications."}),"\n",(0,a.jsx)(n.p,{children:"With this complete system in place, humanoid robots can now respond to natural language commands with intelligent, safe, and context-aware actions, opening up new possibilities for human-robot interaction in both research and practical applications."})]})}function m(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},8453(e,n,t){t.d(n,{R:()=>i,x:()=>r});var s=t(6540);const a={},o=s.createContext(a);function i(e){const n=s.useContext(o);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:i(e.components),s.createElement(o.Provider,{value:n},e.children)}}}]);