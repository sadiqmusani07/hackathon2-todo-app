"use strict";(self.webpackChunkmybookproject=self.webpackChunkmybookproject||[]).push([[2956],{4433(e,n,i){i.r(n),i.d(n,{assets:()=>d,contentTitle:()=>s,default:()=>m,frontMatter:()=>r,metadata:()=>a,toc:()=>c});const a=JSON.parse('{"id":"module-4/voice-to-action","title":"Module 4, Chapter 1: Voice-to-Action with Speech Recognition","description":"Learn to integrate speech recognition with humanoid robots using OpenAI Whisper and ROS 2 messaging","source":"@site/docs/module-4/01-voice-to-action.md","sourceDirName":"module-4","slug":"/module-4/voice-to-action","permalink":"/docs/module-4/voice-to-action","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1,"title":"Module 4, Chapter 1: Voice-to-Action with Speech Recognition","description":"Learn to integrate speech recognition with humanoid robots using OpenAI Whisper and ROS 2 messaging"},"sidebar":"tutorialSidebar","previous":{"title":"Module 3, Chapter 3: Path Planning with Nav2","permalink":"/docs/module-3/nav2-path-planning"},"next":{"title":"Module 4, Chapter 2: Cognitive Planning with Large Language Models","permalink":"/docs/module-4/llm-cognitive-planning"}}');var o=i(4848),t=i(8453);const r={sidebar_position:1,title:"Module 4, Chapter 1: Voice-to-Action with Speech Recognition",description:"Learn to integrate speech recognition with humanoid robots using OpenAI Whisper and ROS 2 messaging"},s="Module 4, Chapter 1: Voice-to-Action with Speech Recognition",d={},c=[{value:"Overview",id:"overview",level:2},{value:"Voice-Based Robot Interaction",id:"voice-based-robot-interaction",level:2},{value:"The Importance of Voice Commands",id:"the-importance-of-voice-commands",level:3},{value:"Voice Processing Pipeline Architecture",id:"voice-processing-pipeline-architecture",level:3},{value:"Voice Command Architecture",id:"voice-command-architecture",level:3},{value:"Design Considerations for Humanoid Robots",id:"design-considerations-for-humanoid-robots",level:3},{value:"OpenAI Whisper Integration",id:"openai-whisper-integration",level:2},{value:"Introduction to Whisper",id:"introduction-to-whisper",level:3},{value:"Setting Up Whisper for Robotics",id:"setting-up-whisper-for-robotics",level:3},{value:"Whisper Model Selection for Robotics",id:"whisper-model-selection-for-robotics",level:3},{value:"Real-time Processing Techniques",id:"real-time-processing-techniques",level:3},{value:"Command Ingestion and Preprocessing",id:"command-ingestion-and-preprocessing",level:2},{value:"Natural Language Command Parsing",id:"natural-language-command-parsing",level:3},{value:"Command Validation and Safety Filtering",id:"command-validation-and-safety-filtering",level:3},{value:"ROS 2 Integration",id:"ros-2-integration",level:2},{value:"Voice Command Publisher",id:"voice-command-publisher",level:3},{value:"Voice Command Subscriber for Execution",id:"voice-command-subscriber-for-execution",level:3},{value:"Performance and Latency Considerations",id:"performance-and-latency-considerations",level:2},{value:"Optimizing Voice Recognition Performance",id:"optimizing-voice-recognition-performance",level:3},{value:"Summary",id:"summary",level:2}];function l(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,t.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"module-4-chapter-1-voice-to-action-with-speech-recognition",children:"Module 4, Chapter 1: Voice-to-Action with Speech Recognition"})}),"\n",(0,o.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,o.jsx)(n.p,{children:"Voice interaction represents a natural and intuitive way for humans to communicate with humanoid robots. This chapter covers how to implement voice-based command systems for humanoid robots using OpenAI Whisper for speech recognition and integrating these capabilities into ROS 2 messaging systems. We'll build a complete pipeline from audio input to actionable commands that can drive humanoid robot behaviors."}),"\n",(0,o.jsx)(n.h2,{id:"voice-based-robot-interaction",children:"Voice-Based Robot Interaction"}),"\n",(0,o.jsx)(n.h3,{id:"the-importance-of-voice-commands",children:"The Importance of Voice Commands"}),"\n",(0,o.jsx)(n.p,{children:"Voice-based interaction is crucial for humanoid robots because it:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Enables natural human-robot communication without requiring specialized interfaces"}),"\n",(0,o.jsx)(n.li,{children:"Allows hands-free operation in various environments"}),"\n",(0,o.jsx)(n.li,{children:"Facilitates collaborative work between humans and robots"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"voice-processing-pipeline-architecture",children:"Voice Processing Pipeline Architecture"}),"\n",(0,o.jsx)(n.p,{children:"This architecture includes audio capture, speech recognition with OpenAI Whisper, natural language processing, and ROS 2 command distribution."}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Provides accessibility for users with mobility limitations"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"voice-command-architecture",children:"Voice Command Architecture"}),"\n",(0,o.jsx)(n.p,{children:"The voice-to-action pipeline follows this flow:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Audio Capture"}),": Microphones capture spoken commands"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Speech Recognition"}),": Convert audio to text using Whisper"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Command Processing"}),": Parse and interpret text commands"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"ROS 2 Integration"}),": Publish commands to ROS 2 topics for execution"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"design-considerations-for-humanoid-robots",children:"Design Considerations for Humanoid Robots"}),"\n",(0,o.jsx)(n.p,{children:"When implementing voice recognition for humanoid robots, consider:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Environmental Noise"}),": Humanoid robots often operate in noisy environments"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Latency Requirements"}),": Responses should feel natural (under 500ms)"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Command Ambiguity"}),": Natural language can be ambiguous"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Safety Constraints"}),": Voice commands must be validated before execution"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Localization"}),": Support for different languages and accents"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"openai-whisper-integration",children:"OpenAI Whisper Integration"}),"\n",(0,o.jsx)(n.h3,{id:"introduction-to-whisper",children:"Introduction to Whisper"}),"\n",(0,o.jsx)(n.p,{children:"OpenAI Whisper is a state-of-the-art speech recognition system trained on 680,000 hours of multilingual and multitask supervised data. For robotics applications, Whisper provides:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"High Accuracy"}),": Robust performance across different accents and environments"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Multiple Languages"}),": Support for 99+ languages"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Real-time Capability"}),": Can process streaming audio"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Open Source"}),": Available under MIT license"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"setting-up-whisper-for-robotics",children:"Setting Up Whisper for Robotics"}),"\n",(0,o.jsx)(n.p,{children:"First, install the required dependencies:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"pip install openai-whisper\n# Or for GPU acceleration:\npip install openai-whisper[cuda]\n"})}),"\n",(0,o.jsx)(n.p,{children:"For robotics applications, we'll use Whisper in a streaming configuration:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import whisper\nimport torch\nimport rospy\nfrom audio_common_msgs.msg import AudioData\nfrom std_msgs.msg import String\nimport numpy as np\nimport pyaudio\nimport wave\n\nclass WhisperVoiceRecognizer:\n    def __init__(self):\n        # Initialize Whisper model\n        # Use \'tiny\' or \'base\' for real-time applications, \'large\' for accuracy\n        self.model = whisper.load_model("base")\n\n        # Initialize PyAudio for audio capture\n        self.audio = pyaudio.PyAudio()\n\n        # Audio parameters\n        self.FORMAT = pyaudio.paInt16\n        self.CHANNELS = 1\n        self.RATE = 16000  # Whisper works best at 16kHz\n        self.CHUNK = 1024\n\n        # ROS initialization\n        rospy.init_node(\'whisper_voice_recognizer\')\n        self.command_pub = rospy.Publisher(\'/voice_commands\', String, queue_size=10)\n\n        # Audio stream\n        self.stream = self.audio.open(\n            format=self.FORMAT,\n            channels=self.CHANNELS,\n            rate=self.RATE,\n            input=True,\n            frames_per_buffer=self.CHUNK\n        )\n\n        rospy.loginfo("Whisper Voice Recognizer initialized")\n\n    def start_listening(self):\n        """Start continuous listening and recognition"""\n        rospy.loginfo("Starting voice recognition...")\n\n        while not rospy.is_shutdown():\n            # Read audio data\n            frames = []\n\n            # Capture a chunk of audio (about 1 second)\n            for _ in range(0, int(self.RATE / self.CHUNK * 1)):\n                data = self.stream.read(self.CHUNK)\n                frames.append(data)\n\n            # Convert to numpy array\n            audio_data = np.frombuffer(b\'\'.join(frames), dtype=np.int16)\n\n            # Convert to float32 and normalize\n            audio_data = audio_data.astype(np.float32) / 32768.0\n\n            # Run Whisper transcription\n            result = self.model.transcribe(audio_data, fp16=torch.cuda.is_available())\n\n            if result["text"].strip():  # Only publish non-empty results\n                command_text = result["text"].strip()\n\n                # Publish recognized command\n                self.command_pub.publish(command_text)\n                rospy.loginfo(f"Recognized: {command_text}")\n\n    def shutdown(self):\n        """Clean shutdown of audio resources"""\n        self.stream.stop_stream()\n        self.stream.close()\n        self.audio.terminate()\n\nif __name__ == \'__main__\':\n    recognizer = WhisperVoiceRecognizer()\n\n    try:\n        recognizer.start_listening()\n    except rospy.ROSInterruptException:\n        pass\n    finally:\n        recognizer.shutdown()\n'})}),"\n",(0,o.jsx)(n.h3,{id:"whisper-model-selection-for-robotics",children:"Whisper Model Selection for Robotics"}),"\n",(0,o.jsx)(n.p,{children:"Different Whisper models offer trade-offs between speed and accuracy:"}),"\n",(0,o.jsxs)(n.table,{children:[(0,o.jsx)(n.thead,{children:(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.th,{children:"Model"}),(0,o.jsx)(n.th,{children:"Size"}),(0,o.jsx)(n.th,{children:"Speed"}),(0,o.jsx)(n.th,{children:"Accuracy"}),(0,o.jsx)(n.th,{children:"Recommended Use Case"})]})}),(0,o.jsxs)(n.tbody,{children:[(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:"tiny"}),(0,o.jsx)(n.td,{children:"75MB"}),(0,o.jsx)(n.td,{children:"Fast"}),(0,o.jsx)(n.td,{children:"Lower"}),(0,o.jsx)(n.td,{children:"Real-time applications with limited compute"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:"base"}),(0,o.jsx)(n.td,{children:"145MB"}),(0,o.jsx)(n.td,{children:"Fast"}),(0,o.jsx)(n.td,{children:"Medium"}),(0,o.jsx)(n.td,{children:"Standard robotics applications"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:"small"}),(0,o.jsx)(n.td,{children:"466MB"}),(0,o.jsx)(n.td,{children:"Medium"}),(0,o.jsx)(n.td,{children:"Good"}),(0,o.jsx)(n.td,{children:"Higher accuracy requirements"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:"medium"}),(0,o.jsx)(n.td,{children:"1.5GB"}),(0,o.jsx)(n.td,{children:"Slow"}),(0,o.jsx)(n.td,{children:"Better"}),(0,o.jsx)(n.td,{children:"Precision applications"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:"large"}),(0,o.jsx)(n.td,{children:"3.0GB"}),(0,o.jsx)(n.td,{children:"Slow"}),(0,o.jsx)(n.td,{children:"Best"}),(0,o.jsx)(n.td,{children:"Offline processing or high-performance systems"})]})]})]}),"\n",(0,o.jsx)(n.p,{children:"For humanoid robots, we typically use 'base' or 'small' models to balance accuracy and response time."}),"\n",(0,o.jsx)(n.h3,{id:"real-time-processing-techniques",children:"Real-time Processing Techniques"}),"\n",(0,o.jsx)(n.p,{children:"For real-time voice recognition, implement these techniques:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import threading\nimport queue\nimport time\n\nclass RealTimeWhisperRecognizer:\n    def __init__(self):\n        self.model = whisper.load_model("base")\n        self.audio_queue = queue.Queue()\n        self.result_queue = queue.Queue()\n\n        # Audio parameters\n        self.FORMAT = pyaudio.paInt16\n        self.CHANNELS = 1\n        self.RATE = 16000\n        self.CHUNK = 1024\n\n        # Buffer for overlapping windows\n        self.buffer_size = int(self.RATE * 2)  # 2-second buffer\n        self.audio_buffer = np.zeros(self.buffer_size, dtype=np.float32)\n        self.buffer_index = 0\n\n        # Threading\n        self.recognition_thread = threading.Thread(target=self.recognition_worker)\n        self.recognition_thread.daemon = True\n\n        # ROS\n        rospy.init_node(\'realtime_whisper_recognizer\')\n        self.command_pub = rospy.Publisher(\'/voice_commands\', String, queue_size=10)\n\n    def start_recognition(self):\n        """Start real-time recognition with overlapping windows"""\n        self.recognition_thread.start()\n\n        # Initialize PyAudio\n        audio = pyaudio.PyAudio()\n        stream = audio.open(\n            format=self.FORMAT,\n            channels=self.CHANNELS,\n            rate=self.RATE,\n            input=True,\n            frames_per_buffer=self.CHUNK\n        )\n\n        try:\n            while not rospy.is_shutdown():\n                # Read audio chunk\n                data = stream.read(self.CHUNK)\n                audio_chunk = np.frombuffer(data, dtype=np.int16).astype(np.float32) / 32768.0\n\n                # Add to circular buffer\n                chunk_len = len(audio_chunk)\n                if self.buffer_index + chunk_len < self.buffer_size:\n                    self.audio_buffer[self.buffer_index:self.buffer_index + chunk_len] = audio_chunk\n                    self.buffer_index += chunk_len\n                else:\n                    # Wrap around the buffer\n                    first_part_len = self.buffer_size - self.buffer_index\n                    self.audio_buffer[self.buffer_index:] = audio_chunk[:first_part_len]\n                    self.audio_buffer[:chunk_len - first_part_len] = audio_chunk[first_part_len:]\n                    self.buffer_index = (self.buffer_index + chunk_len) % self.buffer_size\n\n                # Check for speech activity (simple energy-based VAD)\n                if np.mean(np.abs(audio_chunk)) > 0.01:  # Threshold for speech detection\n                    # Add to recognition queue\n                    start_idx = (self.buffer_index - int(self.RATE * 3)) % self.buffer_size  # 3-second window\n                    audio_window = np.concatenate([\n                        self.audio_buffer[start_idx:],\n                        self.audio_buffer[:start_idx]\n                    ])\n\n                    self.audio_queue.put(audio_window)\n\n                time.sleep(0.01)  # 10ms sleep for 100Hz processing\n\n        finally:\n            stream.stop_stream()\n            stream.close()\n            audio.terminate()\n\n    def recognition_worker(self):\n        """Background thread for recognition"""\n        while not rospy.is_shutdown():\n            try:\n                # Get audio from queue\n                audio_data = self.audio_queue.get(timeout=0.1)\n\n                # Run Whisper transcription\n                result = self.model.transcribe(audio_data, fp16=torch.cuda.is_available())\n\n                if result["text"].strip():\n                    self.command_pub.publish(result["text"].strip())\n\n            except queue.Empty:\n                continue\n            except Exception as e:\n                rospy.logerr(f"Recognition error: {e}")\n'})}),"\n",(0,o.jsx)(n.h2,{id:"command-ingestion-and-preprocessing",children:"Command Ingestion and Preprocessing"}),"\n",(0,o.jsx)(n.h3,{id:"natural-language-command-parsing",children:"Natural Language Command Parsing"}),"\n",(0,o.jsx)(n.p,{children:"Natural language commands need to be parsed and validated before execution:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"import re\nfrom typing import Dict, List, Optional\nfrom dataclasses import dataclass\n\n@dataclass\nclass ParsedCommand:\n    action: str\n    target: Optional[str] = None\n    parameters: Dict[str, str] = None\n\nclass CommandParser:\n    def __init__(self):\n        # Define command patterns\n        self.patterns = {\n            'move': [\n                r'move\\s+(?P<direction>forward|backward|left|right|up|down)\\s*(?P<distance>\\d+(?:\\.\\d+)?)?\\s*(?P<unit>m|cm|mm)?',\n                r'go\\s+(?P<direction>forward|backward|left|right|up|down)\\s*(?P<distance>\\d+(?:\\.\\d+)?)?\\s*(?P<unit>m|cm|mm)?',\n                r'walk\\s+(?P<direction>forward|backward|left|right|up|down)\\s*(?P<distance>\\d+(?:\\.\\d+)?)?\\s*(?P<unit>m|cm|mm)?',\n                r'take\\s+(?P<distance>\\d+(?:\\.\\d+)?)?\\s*(?P<unit>m|cm|mm)?\\s+steps?\\s+(?P<direction>forward|backward|left|right)'\n            ],\n            'navigate': [\n                r'go\\s+to\\s+(?P<location>[a-zA-Z0-9\\s]+)',\n                r'navigate\\s+to\\s+(?P<location>[a-zA-Z0-9\\s]+)',\n                r'move\\s+to\\s+(?P<location>[a-zA-Z0-9\\s]+)',\n                r'walk\\s+to\\s+(?P<location>[a-zA-Z0-9\\s]+)'\n            ],\n            'manipulate': [\n                r'pick\\s+up\\s+(?P<object>[a-zA-Z0-9\\s]+)',\n                r'grasp\\s+(?P<object>[a-zA-Z0-9\\s]+)',\n                r'hold\\s+(?P<object>[a-zA-Z0-9\\s]+)',\n                r'put\\s+(?P<object>[a-zA-Z0-9\\s]+)\\s+(?P<action>down|on|in|into)\\s+(?P<target>[a-zA-Z0-9\\s]+)',\n                r'release\\s+(?P<object>[a-zA-Z0-9\\s]+)'\n            ],\n            'look': [\n                r'look\\s+at\\s+(?P<target>[a-zA-Z0-9\\s]+)',\n                r'face\\s+(?P<target>[a-zA-Z0-9\\s]+)',\n                r'turn\\s+to\\s+face\\s+(?P<target>[a-zA-Z0-9\\s]+)',\n                r'rotate\\s+to\\s+see\\s+(?P<target>[a-zA-Z0-9\\s]+)'\n            ],\n            'speak': [\n                r'say\\s+(?P<message>.+)',\n                r'speak\\s+(?P<message>.+)',\n                r'tell\\s+(?P<message>.+)'\n            ]\n        }\n\n    def parse_command(self, text: str) -> Optional[ParsedCommand]:\n        \"\"\"Parse natural language command into structured format\"\"\"\n        text_lower = text.lower().strip()\n\n        for action, patterns in self.patterns.items():\n            for pattern in patterns:\n                match = re.search(pattern, text_lower)\n                if match:\n                    groups = match.groupdict()\n\n                    # Process units and distances\n                    if 'distance' in groups and 'unit' in groups:\n                        distance = float(groups.get('distance', 1.0))\n                        unit = groups.get('unit', 'm')\n\n                        # Convert to meters\n                        if unit == 'cm':\n                            distance = distance / 100.0\n                        elif unit == 'mm':\n                            distance = distance / 1000.0\n\n                        groups['distance'] = distance\n\n                    # Extract action and parameters\n                    params = {k: v for k, v in groups.items() if v is not None}\n                    target = params.pop('location', params.pop('object', params.pop('target', None)))\n\n                    return ParsedCommand(\n                        action=action,\n                        target=target,\n                        parameters=params\n                    )\n\n        # If no pattern matched, return None\n        return None\n\n# Example usage\nparser = CommandParser()\ncommand = parser.parse_command(\"Move forward 2 meters\")\nprint(f\"Parsed: {command}\")\n"})}),"\n",(0,o.jsx)(n.h3,{id:"command-validation-and-safety-filtering",children:"Command Validation and Safety Filtering"}),"\n",(0,o.jsx)(n.p,{children:"Before executing voice commands, implement validation and safety checks:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"class CommandValidator:\n    def __init__(self):\n        # Define safe movement limits for humanoid robots\n        self.safe_limits = {\n            'max_translation_speed': 0.5,  # m/s\n            'max_rotation_speed': 0.5,     # rad/s\n            'max_distance': 5.0,           # m\n            'max_lift_height': 1.5,        # m (above ground)\n            'safe_zones': ['living_room', 'kitchen', 'office']  # Safe navigation areas\n        }\n\n        # Dangerous commands to filter\n        self.dangerous_patterns = [\n            r'destroy',\n            r'break',\n            r'hurt',\n            r'damage',\n            r'jump from',\n            r'fall down',\n            r'hit',\n            r'crash'\n        ]\n\n    def validate_command(self, parsed_command: ParsedCommand) -> tuple[bool, str]:\n        \"\"\"Validate command for safety and feasibility\"\"\"\n\n        # Check for dangerous commands\n        if self.is_dangerous_command(parsed_command):\n            return False, \"Command contains potentially dangerous actions\"\n\n        # Validate movement commands\n        if parsed_command.action == 'move':\n            if not self.validate_move_command(parsed_command):\n                return False, \"Movement command exceeds safety limits\"\n\n        # Validate navigation commands\n        if parsed_command.action == 'navigate':\n            if not self.validate_navigation_command(parsed_command):\n                return False, \"Navigation destination is not in safe zones\"\n\n        # Validate manipulation commands\n        if parsed_command.action == 'manipulate':\n            if not self.validate_manipulation_command(parsed_command):\n                return False, \"Manipulation command may damage robot or environment\"\n\n        return True, \"Command is valid and safe\"\n\n    def is_dangerous_command(self, parsed_command: ParsedCommand) -> bool:\n        \"\"\"Check if command contains dangerous patterns\"\"\"\n        text = f\"{parsed_command.action} {parsed_command.target} {' '.join(parsed_command.parameters.values()) if parsed_command.parameters else ''}\"\n\n        for pattern in self.dangerous_patterns:\n            if re.search(pattern, text, re.IGNORECASE):\n                return True\n        return False\n\n    def validate_move_command(self, parsed_command: ParsedCommand) -> bool:\n        \"\"\"Validate movement command parameters\"\"\"\n        if parsed_command.parameters:\n            # Check distance limits\n            if 'distance' in parsed_command.parameters:\n                distance = float(parsed_command.parameters['distance'])\n                if distance > self.safe_limits['max_distance']:\n                    return False\n\n            # Check direction constraints (humanoid-specific)\n            direction = parsed_command.parameters.get('direction', '')\n            if direction in ['up', 'down'] and distance > 0.1:  # Humanoids have limited vertical movement\n                return False\n\n        return True\n\n    def validate_navigation_command(self, parsed_command: ParsedCommand) -> bool:\n        \"\"\"Validate navigation destination\"\"\"\n        if parsed_command.target:\n            # Check if destination is in safe zones\n            target_location = parsed_command.target.lower()\n            for safe_zone in self.safe_limits['safe_zones']:\n                if safe_zone in target_location:\n                    return True\n        return False\n\n    def validate_manipulation_command(self, parsed_command: ParsedCommand) -> bool:\n        \"\"\"Validate manipulation command safety\"\"\"\n        if parsed_command.target:\n            # Check for fragile objects that shouldn't be manipulated\n            fragile_objects = ['glass', 'mirror', 'vase', 'window', 'computer']\n            target_lower = parsed_command.target.lower()\n\n            for obj in fragile_objects:\n                if obj in target_lower:\n                    return False\n\n        return True\n"})}),"\n",(0,o.jsx)(n.h2,{id:"ros-2-integration",children:"ROS 2 Integration"}),"\n",(0,o.jsx)(n.h3,{id:"voice-command-publisher",children:"Voice Command Publisher"}),"\n",(0,o.jsx)(n.p,{children:"Create a ROS 2 node that publishes recognized voice commands:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Twist\nfrom sensor_msgs.msg import JointState\nimport threading\nimport queue\n\nclass VoiceCommandPublisher(Node):\n    def __init__(self):\n        super().__init__(\'voice_command_publisher\')\n\n        # Publisher for raw voice commands\n        self.raw_command_pub = self.create_publisher(String, \'/voice/raw_command\', 10)\n\n        # Publisher for parsed commands\n        self.parsed_command_pub = self.create_publisher(String, \'/voice/parsed_command\', 10)\n\n        # Publisher for validated commands\n        self.validated_command_pub = self.create_publisher(String, \'/voice/validated_command\', 10)\n\n        # Command queue for processing\n        self.command_queue = queue.Queue()\n\n        # Command parser and validator\n        self.parser = CommandParser()\n        self.validator = CommandValidator()\n\n        # Start command processing thread\n        self.processing_thread = threading.Thread(target=self.process_commands)\n        self.processing_thread.daemon = True\n        self.processing_thread.start()\n\n        self.get_logger().info("Voice Command Publisher initialized")\n\n    def publish_raw_command(self, text: str):\n        """Publish raw recognized text"""\n        msg = String()\n        msg.data = text\n        self.raw_command_pub.publish(msg)\n\n    def process_command(self, text: str):\n        """Process voice command through parsing and validation pipeline"""\n        # Publish raw command\n        self.publish_raw_command(text)\n\n        # Parse command\n        parsed = self.parser.parse_command(text)\n        if parsed:\n            # Publish parsed command\n            parsed_msg = String()\n            parsed_msg.data = f"{parsed.action}:{parsed.target or \'None\'}:{parsed.parameters or {}}"\n            self.parsed_command_pub.publish(parsed_msg)\n\n            # Validate command\n            is_valid, reason = self.validator.validate_command(parsed)\n            if is_valid:\n                # Publish validated command\n                validated_msg = String()\n                validated_msg.data = f"{parsed.action}:{parsed.target or \'None\'}:{parsed.parameters or {}}"\n                self.validated_command_pub.publish(validated_msg)\n\n                self.get_logger().info(f"Valid command processed: {text}")\n            else:\n                self.get_logger().warn(f"Invalid command rejected: {text} ({reason})")\n        else:\n            self.get_logger().info(f"No command parsed from: {text}")\n\n    def process_commands(self):\n        """Background thread to process commands from queue"""\n        while rclpy.ok():\n            try:\n                text = self.command_queue.get(timeout=0.1)\n                self.process_command(text)\n            except queue.Empty:\n                continue\n\n    def add_command_to_queue(self, text: str):\n        """Add command to processing queue"""\n        self.command_queue.put(text)\n\ndef main(args=None):\n    rclpy.init(args=args)\n\n    voice_publisher = VoiceCommandPublisher()\n\n    # Example: Simulate receiving voice commands\n    # In real implementation, this would come from Whisper recognition\n    sample_commands = [\n        "Move forward 1 meter",\n        "Go to kitchen",\n        "Look at the table",\n        "Pick up the red cup"\n    ]\n\n    for cmd in sample_commands:\n        voice_publisher.add_command_to_queue(cmd)\n        import time\n        time.sleep(1)  # Simulate timing between commands\n\n    try:\n        rclpy.spin(voice_publisher)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        voice_publisher.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,o.jsx)(n.h3,{id:"voice-command-subscriber-for-execution",children:"Voice Command Subscriber for Execution"}),"\n",(0,o.jsx)(n.p,{children:"Create a subscriber that receives validated commands and executes them:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Twist\nfrom sensor_msgs.msg import JointState\nfrom nav_msgs.msg import Path\nfrom builtin_interfaces.msg import Duration\nimport json\n\nclass VoiceCommandExecutor(Node):\n    def __init__(self):\n        super().__init__(\'voice_command_executor\')\n\n        # Subscriber for validated commands\n        self.command_sub = self.create_subscription(\n            String,\n            \'/voice/validated_command\',\n            self.command_callback,\n            10\n        )\n\n        # Publishers for robot control\n        self.cmd_vel_pub = self.create_publisher(Twist, \'/cmd_vel\', 10)\n        self.joint_cmd_pub = self.create_publisher(JointState, \'/joint_commands\', 10)\n        self.nav_goal_pub = self.create_publisher(Path, \'/move_base_simple/goal\', 10)\n\n        # Robot state\n        self.current_pose = None\n        self.robot_joints = {}\n\n        self.get_logger().info("Voice Command Executor initialized")\n\n    def command_callback(self, msg):\n        """Process validated voice command"""\n        try:\n            # Parse command from string format: action:target:params_dict\n            parts = msg.data.split(\':\', 2)\n            if len(parts) >= 2:\n                action = parts[0]\n                target = parts[1] if parts[1] != \'None\' else None\n                params_str = parts[2] if len(parts) > 2 else \'{}\'\n\n                # Convert params string back to dict\n                try:\n                    params = json.loads(params_str.replace("\'", \'"\'))\n                except:\n                    params = {}\n\n                self.execute_command(action, target, params)\n            else:\n                self.get_logger().warn(f"Invalid command format: {msg.data}")\n\n        except Exception as e:\n            self.get_logger().error(f"Error processing command {msg.data}: {e}")\n\n    def execute_command(self, action: str, target: str, params: dict):\n        """Execute the parsed command"""\n        self.get_logger().info(f"Executing command: {action} with target {target} and params {params}")\n\n        if action == \'move\':\n            self.execute_move_command(params)\n        elif action == \'navigate\':\n            self.execute_navigate_command(target)\n        elif action == \'manipulate\':\n            self.execute_manipulate_command(target, params)\n        elif action == \'look\':\n            self.execute_look_command(target)\n        else:\n            self.get_logger().warn(f"Unknown action: {action}")\n\n    def execute_move_command(self, params: dict):\n        """Execute movement command"""\n        direction = params.get(\'direction\', \'forward\')\n        distance = float(params.get(\'distance\', 1.0))\n\n        # Create velocity command based on direction\n        cmd_vel = Twist()\n\n        if direction == \'forward\':\n            cmd_vel.linear.x = 0.3  # m/s\n        elif direction == \'backward\':\n            cmd_vel.linear.x = -0.3\n        elif direction == \'left\':\n            cmd_vel.linear.y = 0.3\n        elif direction == \'right\':\n            cmd_vel.linear.y = -0.3\n        elif direction == \'up\':\n            cmd_vel.linear.z = 0.1\n        elif direction == \'down\':\n            cmd_vel.linear.z = -0.1\n\n        # Publish command for duration based on distance\n        duration = distance / 0.3  # assuming 0.3 m/s speed\n        self.move_for_duration(cmd_vel, duration)\n\n    def move_for_duration(self, cmd_vel: Twist, duration: float):\n        """Move robot with specified velocity for given duration"""\n        start_time = self.get_clock().now()\n        end_time = start_time + Duration(sec=int(duration), nanosec=int((duration % 1) * 1e9))\n\n        while self.get_clock().now() < end_time and rclpy.ok():\n            self.cmd_vel_pub.publish(cmd_vel)\n            self.get_clock().sleep_for(Duration(nanosec=100000000))  # 100ms sleep\n\n    def execute_navigate_command(self, target: str):\n        """Execute navigation command"""\n        if target:\n            # In a real implementation, this would convert target location\n            # to coordinates and send to navigation stack\n            self.get_logger().info(f"Navigating to: {target}")\n            # Implementation would go here\n\n    def execute_manipulate_command(self, target: str, params: dict):\n        """Execute manipulation command"""\n        if target:\n            self.get_logger().info(f"Manipulating: {target}")\n            # Implementation for gripper control, arm movement, etc.\n            # Would use joint control or manipulation actions\n\n    def execute_look_command(self, target: str):\n        """Execute look command"""\n        if target:\n            self.get_logger().info(f"Looking at: {target}")\n            # Implementation for head/neck joint control\n            # to orient camera toward target\n\ndef main(args=None):\n    rclpy.init(args=args)\n\n    executor = VoiceCommandExecutor()\n\n    try:\n        rclpy.spin(executor)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        executor.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,o.jsx)(n.h2,{id:"performance-and-latency-considerations",children:"Performance and Latency Considerations"}),"\n",(0,o.jsx)(n.h3,{id:"optimizing-voice-recognition-performance",children:"Optimizing Voice Recognition Performance"}),"\n",(0,o.jsx)(n.p,{children:"For real-time humanoid applications, optimize for latency:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# Optimized voice recognition node with performance considerations\nimport rclpy\nfrom rclpy.node import Node\nfrom audio_common_msgs.msg import AudioData\nfrom std_msgs.msg import String\nimport numpy as np\nimport threading\nimport time\nfrom collections import deque\n\nclass OptimizedVoiceRecognizer(Node):\n    def __init__(self):\n        super().__init__(\'optimized_voice_recognizer\')\n\n        # Performance parameters\n        self.processing_interval = 0.1  # Process every 100ms\n        self.audio_buffer_size = 4096  # Small buffer for low latency\n        self.min_speech_energy = 0.01  # Minimum energy for speech detection\n\n        # Audio buffer for overlapping windows\n        self.audio_buffer = deque(maxlen=16000 * 2)  # 2 seconds of audio at 16kHz\n\n        # Publishers\n        self.command_pub = self.create_publisher(String, \'/voice_commands\', 10)\n\n        # Audio subscriber\n        self.audio_sub = self.create_subscription(\n            AudioData,\n            \'/audio_input\',\n            self.audio_callback,\n            10\n        )\n\n        # Threading for processing\n        self.processing_lock = threading.Lock()\n        self.last_process_time = time.time()\n\n        # Initialize Whisper model (use smaller model for faster processing)\n        import whisper\n        self.model = whisper.load_model("tiny")  # Fastest model\n\n        self.get_logger().info("Optimized Voice Recognizer initialized")\n\n    def audio_callback(self, msg):\n        """Process incoming audio data"""\n        # Convert audio data to numpy array\n        audio_data = np.frombuffer(msg.data, dtype=np.int16).astype(np.float32) / 32768.0\n\n        # Add to buffer\n        for sample in audio_data:\n            self.audio_buffer.append(sample)\n\n        # Check if we have enough audio and if it\'s above threshold\n        if len(self.audio_buffer) >= 16000:  # At least 1 second of audio\n            current_time = time.time()\n\n            # Check for speech activity\n            audio_array = np.array(list(self.audio_buffer)[-16000:])  # Last 1 second\n            energy = np.mean(np.abs(audio_array))\n\n            if energy > self.min_speech_energy and (current_time - self.last_process_time) > self.processing_interval:\n                # Process audio for recognition\n                with self.processing_lock:\n                    self.process_audio_segment(audio_array)\n                    self.last_process_time = current_time\n\n    def process_audio_segment(self, audio_segment):\n        """Process audio segment with Whisper"""\n        try:\n            # Run Whisper transcription\n            result = self.model.transcribe(audio_segment, fp16=False)  # Use fp32 for consistency\n\n            if result["text"].strip():\n                # Publish recognized command\n                cmd_msg = String()\n                cmd_msg.data = result["text"].strip()\n                self.command_pub.publish(cmd_msg)\n\n                self.get_logger().info(f"Recognized: {cmd_msg.data}")\n\n        except Exception as e:\n            self.get_logger().error(f"Error in audio processing: {e}")\n\ndef main(args=None):\n    rclpy.init(args=args)\n\n    recognizer = OptimizedVoiceRecognizer()\n\n    try:\n        rclpy.spin(recognizer)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        recognizer.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,o.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,o.jsx)(n.p,{children:"In this chapter, you've learned how to:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Set up OpenAI Whisper for real-time voice recognition in robotics applications"}),"\n",(0,o.jsx)(n.li,{children:"Parse natural language commands for humanoid robot execution"}),"\n",(0,o.jsx)(n.li,{children:"Validate voice commands for safety and feasibility"}),"\n",(0,o.jsx)(n.li,{children:"Integrate voice recognition with ROS 2 messaging systems"}),"\n",(0,o.jsx)(n.li,{children:"Optimize for performance and low-latency responses"}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:["In the ",(0,o.jsx)(n.a,{href:"./02-llm-cognitive-planning",children:"next chapter"}),", we'll explore how to implement cognitive planning using large language models to translate natural language goals into executable action plans for humanoid robots."]})]})}function m(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(l,{...e})}):l(e)}},8453(e,n,i){i.d(n,{R:()=>r,x:()=>s});var a=i(6540);const o={},t=a.createContext(o);function r(e){const n=a.useContext(t);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:r(e.components),a.createElement(t.Provider,{value:n},e.children)}}}]);