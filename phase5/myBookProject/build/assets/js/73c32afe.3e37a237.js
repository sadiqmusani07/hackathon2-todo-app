"use strict";(self.webpackChunkmybookproject=self.webpackChunkmybookproject||[]).push([[7203],{779(e,n,t){t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>d,frontMatter:()=>i,metadata:()=>a,toc:()=>c});const a=JSON.parse('{"id":"module-4/llm-cognitive-planning","title":"Module 4, Chapter 2: Cognitive Planning with Large Language Models","description":"Learn to implement cognitive planning using LLMs to translate natural language goals into action plans for humanoid robots","source":"@site/docs/module-4/02-llm-cognitive-planning.md","sourceDirName":"module-4","slug":"/module-4/llm-cognitive-planning","permalink":"/docs/module-4/llm-cognitive-planning","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2,"title":"Module 4, Chapter 2: Cognitive Planning with Large Language Models","description":"Learn to implement cognitive planning using LLMs to translate natural language goals into action plans for humanoid robots"},"sidebar":"tutorialSidebar","previous":{"title":"Module 4, Chapter 1: Voice-to-Action with Speech Recognition","permalink":"/docs/module-4/voice-to-action"},"next":{"title":"Module 4, Chapter 3: Capstone - The Autonomous Humanoid","permalink":"/docs/module-4/autonomous-humanoid"}}');var o=t(4848),s=t(8453);const i={sidebar_position:2,title:"Module 4, Chapter 2: Cognitive Planning with Large Language Models",description:"Learn to implement cognitive planning using LLMs to translate natural language goals into action plans for humanoid robots"},r="Module 4, Chapter 2: Cognitive Planning with Large Language Models",l={},c=[{value:"Overview",id:"overview",level:2},{value:"Role of LLMs in Robotic Reasoning",id:"role-of-llms-in-robotic-reasoning",level:2},{value:"Cognitive Architecture for Humanoid Robots",id:"cognitive-architecture-for-humanoid-robots",level:3},{value:"LLM Integration Patterns",id:"llm-integration-patterns",level:3},{value:"Safety and Grounding Considerations",id:"safety-and-grounding-considerations",level:3},{value:"Translating Natural Language Goals to Action Plans",id:"translating-natural-language-goals-to-action-plans",level:2},{value:"Natural Language Understanding Pipeline",id:"natural-language-understanding-pipeline",level:3},{value:"Context-Aware Planning",id:"context-aware-planning",level:3},{value:"Mapping Symbolic Plans to ROS 2 Actions",id:"mapping-symbolic-plans-to-ros-2-actions",level:2},{value:"ROS 2 Action Integration",id:"ros-2-action-integration",level:3},{value:"Safety Validation Layer",id:"safety-validation-layer",level:3},{value:"Practical Exercise: Complete LLM-Based Planning System",id:"practical-exercise-complete-llm-based-planning-system",level:2},{value:"Summary",id:"summary",level:2}];function p(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"module-4-chapter-2-cognitive-planning-with-large-language-models",children:"Module 4, Chapter 2: Cognitive Planning with Large Language Models"})}),"\n",(0,o.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,o.jsx)(n.p,{children:"Large Language Models (LLMs) represent a breakthrough in artificial intelligence that enables natural language understanding and reasoning capabilities. In this chapter, we'll explore how to implement cognitive planning for humanoid robots using LLMs to translate natural language goals into executable action plans. We'll cover the role of LLMs in robotic reasoning, safety considerations, and integration with ROS 2 for actionable robot commands."}),"\n",(0,o.jsx)(n.h2,{id:"role-of-llms-in-robotic-reasoning",children:"Role of LLMs in Robotic Reasoning"}),"\n",(0,o.jsx)(n.h3,{id:"cognitive-architecture-for-humanoid-robots",children:"Cognitive Architecture for Humanoid Robots"}),"\n",(0,o.jsx)(n.p,{children:"LLMs serve as the cognitive layer in humanoid robots, bridging the gap between high-level natural language commands and low-level motor actions. This cognitive layer handles:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Natural Language Understanding"}),": Interpreting human commands and requests"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Task Decomposition"}),": Breaking complex goals into executable subtasks"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Knowledge Integration"}),": Incorporating world knowledge and commonsense reasoning"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Plan Synthesis"}),": Creating action sequences that achieve desired goals"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Context Awareness"}),": Understanding situational context and environmental constraints"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"llm-integration-patterns",children:"LLM Integration Patterns"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# Example: LLM integration for robotic reasoning\nimport openai\nimport json\nimport asyncio\nfrom typing import Dict, List, Optional\nfrom dataclasses import dataclass\n\n@dataclass\nclass ActionPlan:\n    """Structured representation of robot action plan"""\n    id: str\n    goal: str\n    steps: List[Dict]\n    constraints: List[str]\n    estimated_duration: float\n\nclass LLMBasedCognitivePlanner:\n    def __init__(self, api_key: str, model: str = "gpt-4"):\n        self.client = openai.OpenAI(api_key=api_key)\n        self.model = model\n        self.robot_capabilities = self._load_robot_capabilities()\n\n    def _load_robot_capabilities(self) -> Dict:\n        """Load robot-specific capabilities and constraints"""\n        return {\n            "locomotion": {\n                "max_speed": 0.5,  # m/s\n                "step_height": 0.1,  # m\n                "turning_radius": 0.3,  # m\n                "balance_constraints": True\n            },\n            "manipulation": {\n                "reach": 1.2,  # m\n                "grip_force": 50,  # N\n                "precision_modes": ["fine", "coarse", "power"]\n            },\n            "perception": {\n                "lidar_range": 10.0,  # m\n                "camera_resolution": [640, 480],\n                "detection_range": 5.0  # m\n            },\n            "navigation": {\n                "obstacle_detection": True,\n                "path_planning": True,\n                "localization_accuracy": 0.05  # m\n            }\n        }\n\n    def generate_action_plan(self, goal: str, context: Optional[Dict] = None) -> ActionPlan:\n        """Generate action plan from natural language goal using LLM"""\n\n        # Construct system prompt with robot capabilities\n        system_prompt = f"""\n        You are a cognitive planning system for a humanoid robot. Your role is to:\n        1. Interpret natural language goals\n        2. Decompose complex goals into executable subtasks\n        3. Consider the robot\'s physical capabilities and limitations\n        4. Generate safe and feasible action plans\n        5. Include error handling and recovery strategies\n\n        Robot capabilities:\n        - Locomotion: Max speed {self.robot_capabilities[\'locomotion\'][\'max_speed\']}m/s, step height {self.robot_capabilities[\'locomotion\'][\'step_height\']}m\n        - Manipulation: Reach {self.robot_capabilities[\'manipulation\'][\'reach\']}m, grip force {self.robot_capabilities[\'manipulation\'][\'grip_force\']}N\n        - Perception: LiDAR range {self.robot_capabilities[\'perception\'][\'lidar_range\']}m, camera resolution {self.robot_capabilities[\'perception\'][\'camera_resolution\']}\n        - Navigation: Obstacle detection, path planning, localization accuracy {self.robot_capabilities[\'navigation\'][\'localization_accuracy\']}m\n\n        Constraints:\n        - Always prioritize safety\n        - Consider balance and stability for humanoid movement\n        - Respect physical limitations of the robot\n        - Handle ambiguous or impossible requests gracefully\n\n        Respond with a JSON object containing:\n        {{\n            "id": "unique_plan_id",\n            "goal": "original goal text",\n            "steps": [\n                {{\n                    "id": "step_id",\n                    "action": "action_type",\n                    "parameters": {{"param1": "value1", ...}},\n                    "expected_outcome": "description of expected outcome",\n                    "safety_check": "safety validation to perform",\n                    "recovery_plan": "what to do if step fails"\n                }}\n            ],\n            "constraints": ["list of safety constraints"],\n            "estimated_duration": "estimated time in seconds"\n        }}\n        """\n\n        # Construct user message with goal and context\n        user_message = f"Goal: {goal}\\n\\nContext: {json.dumps(context or {})}"\n\n        try:\n            response = self.client.chat.completions.create(\n                model=self.model,\n                messages=[\n                    {"role": "system", "content": system_prompt},\n                    {"role": "user", "content": user_message}\n                ],\n                temperature=0.1,  # Low temperature for consistency\n                max_tokens=2048,\n                response_format={"type": "json_object"}\n            )\n\n            # Parse response\n            plan_data = json.loads(response.choices[0].message.content)\n            return ActionPlan(**plan_data)\n\n        except Exception as e:\n            raise RuntimeError(f"Failed to generate action plan: {e}")\n\n    def validate_plan_safety(self, plan: ActionPlan) -> tuple[bool, List[str]]:\n        """Validate plan for safety and feasibility"""\n        issues = []\n\n        # Check each step for safety\n        for step in plan.steps:\n            action = step.get(\'action\', \'\')\n            params = step.get(\'parameters\', {})\n\n            # Validate locomotion safety\n            if action in [\'move\', \'navigate\', \'walk\']:\n                if params.get(\'speed\', 0) > self.robot_capabilities[\'locomotion\'][\'max_speed\']:\n                    issues.append(f"Step {step[\'id\']}: Movement speed exceeds maximum capability")\n\n            # Validate manipulation safety\n            elif action in [\'grasp\', \'manipulate\', \'lift\']:\n                if params.get(\'force\', 0) > self.robot_capabilities[\'manipulation\'][\'grip_force\']:\n                    issues.append(f"Step {step[\'id\']}: Grip force exceeds maximum capability")\n\n        return len(issues) == 0, issues\n'})}),"\n",(0,o.jsx)(n.h3,{id:"safety-and-grounding-considerations",children:"Safety and Grounding Considerations"}),"\n",(0,o.jsx)(n.p,{children:"LLM-driven robot control requires careful safety measures:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"# Example: Safety and grounding for LLM-based robot control\nclass SafetyGroundingLayer:\n    def __init__(self):\n        self.forbidden_actions = [\n            'jump', 'fly', 'break', 'damage', 'hurt', 'destroy',\n            'self_harm', 'harm_others', 'violate_privacy'\n        ]\n\n        self.physical_constraints = {\n            'max_height': 2.0,  # Robot cannot reach above this height\n            'min_distance': 0.1,  # Minimum safe distance from obstacles\n            'max_force': 100.0,   # Maximum force for any action\n            'max_velocity': 1.0   # Maximum velocity for any movement\n        }\n\n        self.ethical_constraints = [\n            \"respect_persons\",\n            \"maintain_privacy\",\n            \"follow_social_norms\",\n            \"avoid_harm\"\n        ]\n\n    def ground_llm_output(self, llm_output: Dict) -> tuple[bool, str, Dict]:\n        \"\"\"Apply safety and grounding constraints to LLM output\"\"\"\n\n        # Check for forbidden actions\n        for step in llm_output.get('steps', []):\n            action = step.get('action', '').lower()\n\n            if action in self.forbidden_actions:\n                return False, f\"Forbidden action detected: {action}\", {}\n\n            # Validate physical constraints\n            params = step.get('parameters', {})\n\n            if 'height' in params and params['height'] > self.physical_constraints['max_height']:\n                return False, f\"Height constraint violation: {params['height']}m\", {}\n\n            if 'distance' in params and params['distance'] < self.physical_constraints['min_distance']:\n                return False, f\"Distance constraint violation: {params['distance']}m\", {}\n\n            if 'force' in params and params['force'] > self.physical_constraints['max_force']:\n                return False, f\"Force constraint violation: {params['force']}N\", {}\n\n            if 'velocity' in params and params['velocity'] > self.physical_constraints['max_velocity']:\n                return False, f\"Velocity constraint violation: {params['velocity']}m/s\", {}\n\n        # Apply ethical constraints\n        grounded_output = self._apply_ethical_grounding(llm_output)\n\n        return True, \"Output is safe and grounded\", grounded_output\n\n    def _apply_ethical_grounding(self, output: Dict) -> Dict:\n        \"\"\"Apply ethical grounding to LLM output\"\"\"\n        # Remove any actions that violate ethical constraints\n        filtered_steps = []\n\n        for step in output.get('steps', []):\n            action_desc = f\"{step.get('action', '')} {step.get('parameters', '')}\".lower()\n\n            # Check if action respects persons\n            if any(person_term in action_desc for person_term in ['person', 'human', 'individual']):\n                # Add safety checks for human interaction\n                if 'approach' in action_desc or 'move_to' in action_desc:\n                    step.setdefault('safety_checks', []).append('maintain_safe_distance')\n\n            # Check if action maintains privacy\n            if any(privacy_term in action_desc for privacy_term in ['record', 'capture', 'take_photo', 'observe_private']):\n                step.setdefault('safety_checks', []).append('request_consent')\n\n            filtered_steps.append(step)\n\n        output['steps'] = filtered_steps\n        return output\n"})}),"\n",(0,o.jsx)(n.h2,{id:"translating-natural-language-goals-to-action-plans",children:"Translating Natural Language Goals to Action Plans"}),"\n",(0,o.jsx)(n.h3,{id:"natural-language-understanding-pipeline",children:"Natural Language Understanding Pipeline"}),"\n",(0,o.jsx)(n.p,{children:"The process of translating natural language to action plans involves several steps:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"# Example: Natural language to action plan translation\nimport re\nfrom typing import Dict, Any\n\nclass NaturalLanguageToAction:\n    def __init__(self):\n        # Define action mappings\n        self.action_mappings = {\n            'move': ['move', 'go', 'walk', 'step', 'travel', 'navigate'],\n            'rotate': ['turn', 'rotate', 'spin', 'pivot', 'face'],\n            'manipulate': ['pick', 'grasp', 'grab', 'hold', 'lift', 'place', 'put', 'release'],\n            'perceive': ['look', 'see', 'find', 'locate', 'detect', 'observe', 'examine'],\n            'communicate': ['say', 'speak', 'tell', 'announce', 'report', 'answer'],\n            'navigate': ['goto', 'go_to', 'navigate_to', 'reach', 'arrive_at']\n        }\n\n        # Location keywords\n        self.location_keywords = [\n            'kitchen', 'bedroom', 'living room', 'office', 'bathroom',\n            'table', 'chair', 'door', 'window', 'couch', 'desk'\n        ]\n\n        # Object keywords\n        self.object_keywords = [\n            'cup', 'bottle', 'book', 'phone', 'computer', 'plate',\n            'fork', 'knife', 'spoon', 'box', 'ball', 'toy'\n        ]\n\n    def parse_goal(self, goal_text: str) -> Dict[str, Any]:\n        \"\"\"Parse natural language goal into structured format\"\"\"\n        goal_text_lower = goal_text.lower()\n\n        # Extract action\n        action = self._extract_action(goal_text_lower)\n\n        # Extract target/object\n        target = self._extract_target(goal_text_lower)\n\n        # Extract location\n        location = self._extract_location(goal_text_lower)\n\n        # Extract parameters (quantities, directions, etc.)\n        parameters = self._extract_parameters(goal_text_lower)\n\n        return {\n            'action': action,\n            'target': target,\n            'location': location,\n            'parameters': parameters,\n            'original_text': goal_text\n        }\n\n    def _extract_action(self, text: str) -> str:\n        \"\"\"Extract primary action from text\"\"\"\n        for action_type, keywords in self.action_mappings.items():\n            for keyword in keywords:\n                if keyword in text:\n                    return action_type\n        return 'unknown'\n\n    def _extract_target(self, text: str) -> str:\n        \"\"\"Extract target object from text\"\"\"\n        for obj in self.object_keywords:\n            if obj in text:\n                return obj\n        return None\n\n    def _extract_location(self, text: str) -> str:\n        \"\"\"Extract location from text\"\"\"\n        for loc in self.location_keywords:\n            if loc in text.replace('_', ' '):\n                return loc\n        return None\n\n    def _extract_parameters(self, text: str) -> Dict[str, Any]:\n        \"\"\"Extract numerical and spatial parameters\"\"\"\n        params = {}\n\n        # Extract distances (numbers followed by distance units)\n        distance_pattern = r'(\\d+(?:\\.\\d+)?)\\s*(m|cm|mm|meter|feet|ft)'\n        distances = re.findall(distance_pattern, text)\n        if distances:\n            params['distance'] = float(distances[0][0])\n            params['unit'] = distances[0][1]\n\n        # Extract directions\n        directions = ['forward', 'backward', 'left', 'right', 'up', 'down']\n        for direction in directions:\n            if direction in text:\n                params['direction'] = direction\n\n        # Extract speeds\n        speed_pattern = r'(\\d+(?:\\.\\d+)?)\\s*(m/s|cm/s|mph|kph)'\n        speeds = re.findall(speed_pattern, text)\n        if speeds:\n            params['speed'] = float(speeds[0][0])\n            params['speed_unit'] = speeds[0][1]\n\n        return params\n\n    def convert_to_action_plan(self, parsed_goal: Dict) -> ActionPlan:\n        \"\"\"Convert parsed goal to executable action plan\"\"\"\n\n        # Create action steps based on parsed goal\n        steps = []\n\n        if parsed_goal['action'] == 'navigate':\n            steps.extend(self._create_navigation_steps(parsed_goal))\n        elif parsed_goal['action'] == 'manipulate':\n            steps.extend(self._create_manipulation_steps(parsed_goal))\n        elif parsed_goal['action'] == 'perceive':\n            steps.extend(self._create_perception_steps(parsed_goal))\n        else:\n            steps.extend(self._create_generic_steps(parsed_goal))\n\n        return ActionPlan(\n            id=f\"plan_{hash(str(parsed_goal)) % 10000}\",\n            goal=parsed_goal['original_text'],\n            steps=steps,\n            constraints=[],\n            estimated_duration=len(steps) * 2.0  # 2 seconds per step estimate\n        )\n\n    def _create_navigation_steps(self, parsed_goal: Dict) -> List[Dict]:\n        \"\"\"Create navigation-specific action steps\"\"\"\n        steps = []\n\n        # Look for target location in goal\n        if parsed_goal.get('location'):\n            steps.append({\n                'id': 'nav_find_target',\n                'action': 'locate',\n                'parameters': {'target': parsed_goal['location']},\n                'expected_outcome': f'Location {parsed_goal[\"location\"]} identified',\n                'safety_check': 'validate_location_accessibility',\n                'recovery_plan': 'find_alternative_route'\n            })\n\n            steps.append({\n                'id': 'nav_move_to_target',\n                'action': 'navigate',\n                'parameters': {\n                    'destination': parsed_goal['location'],\n                    'speed': 0.3,\n                    'avoid_obstacles': True\n                },\n                'expected_outcome': f'Reached {parsed_goal[\"location\"]}',\n                'safety_check': 'maintain_balance',\n                'recovery_plan': 'stop_and_replan'\n            })\n\n        return steps\n\n    def _create_manipulation_steps(self, parsed_goal: Dict) -> List[Dict]:\n        \"\"\"Create manipulation-specific action steps\"\"\"\n        steps = []\n\n        if parsed_goal.get('target'):\n            # First locate the object\n            steps.append({\n                'id': 'manip_locate_object',\n                'action': 'locate',\n                'parameters': {'target': parsed_goal['target']},\n                'expected_outcome': f'{parsed_goal[\"target\"]} located',\n                'safety_check': 'validate_object_graspability',\n                'recovery_plan': 'abort_manipulation'\n            })\n\n            # Then approach the object\n            steps.append({\n                'id': 'manip_approach_object',\n                'action': 'move',\n                'parameters': {\n                    'target': parsed_goal['target'],\n                    'distance': 0.3,  # 30cm from object\n                    'speed': 0.1\n                },\n                'expected_outcome': f'Close to {parsed_goal[\"target\"]}',\n                'safety_check': 'validate_approach_safety',\n                'recovery_plan': 'stop_approach'\n            })\n\n            # Finally manipulate the object\n            steps.append({\n                'id': 'manip_execute',\n                'action': 'manipulate',\n                'parameters': {\n                    'target': parsed_goal['target'],\n                    'operation': 'grasp'  # Default operation\n                },\n                'expected_outcome': f'{parsed_goal[\"target\"]} manipulated',\n                'safety_check': 'validate_manipulation_safety',\n                'recovery_plan': 'release_and_retreat'\n            })\n\n        return steps\n"})}),"\n",(0,o.jsx)(n.h3,{id:"context-aware-planning",children:"Context-Aware Planning"}),"\n",(0,o.jsx)(n.p,{children:"Humanoid robots need to consider context when planning:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"# Example: Context-aware planning for humanoid robots\nclass ContextAwarePlanner:\n    def __init__(self):\n        self.context_memory = {}\n        self.environment_map = {}  # Current environment state\n        self.robot_state = {}      # Current robot state\n        self.social_context = {}   # Social situation awareness\n\n    def plan_with_context(self, goal: str, current_context: Dict) -> ActionPlan:\n        \"\"\"Generate action plan considering current context\"\"\"\n\n        # Update context memory\n        self._update_context_memory(current_context)\n\n        # Enhance goal with contextual information\n        enhanced_goal = self._enhance_goal_with_context(goal, current_context)\n\n        # Generate plan with context-aware LLM prompting\n        plan = self._generate_contextual_plan(enhanced_goal)\n\n        # Apply context-specific constraints\n        constrained_plan = self._apply_context_constraints(plan, current_context)\n\n        return constrained_plan\n\n    def _update_context_memory(self, context: Dict):\n        \"\"\"Update context memory with new information\"\"\"\n        self.context_memory.update(context)\n\n        # Update environment map\n        if 'objects' in context:\n            self.environment_map.update(context['objects'])\n\n        # Update robot state\n        if 'robot_state' in context:\n            self.robot_state.update(context['robot_state'])\n\n        # Update social context\n        if 'social' in context:\n            self.social_context.update(context['social'])\n\n    def _enhance_goal_with_context(self, goal: str, context: Dict) -> str:\n        \"\"\"Enhance goal with contextual information\"\"\"\n        enhanced_parts = [goal]\n\n        # Add environment context\n        if 'location' in context:\n            enhanced_parts.append(f\"Current location: {context['location']}\")\n\n        if 'time_of_day' in context:\n            enhanced_parts.append(f\"Time of day: {context['time_of_day']}\")\n\n        if 'people_present' in context:\n            people = ', '.join(context['people_present'])\n            enhanced_parts.append(f\"People present: {people}\")\n\n        # Add robot state context\n        if 'battery_level' in context:\n            enhanced_parts.append(f\"Current battery level: {context['battery_level']}%\")\n\n        if 'current_pose' in context:\n            pose = context['current_pose']\n            enhanced_parts.append(f\"Current pose: x={pose['x']}, y={pose['y']}, theta={pose['theta']}\")\n\n        return '\\n'.join(enhanced_parts)\n\n    def _generate_contextual_plan(self, enhanced_goal: str) -> ActionPlan:\n        \"\"\"Generate plan considering context using LLM\"\"\"\n        # This would call the LLM with enhanced context\n        # For brevity, returning a simplified plan\n        return self._simple_contextual_plan(enhanced_goal)\n\n    def _apply_context_constraints(self, plan: ActionPlan, context: Dict) -> ActionPlan:\n        \"\"\"Apply context-specific constraints to the plan\"\"\"\n\n        # Modify plan based on battery level\n        if context.get('battery_level', 100) < 20:\n            plan.constraints.append(\"Conserve energy: avoid unnecessary movements\")\n            plan.constraints.append(\"Prioritize charging station navigation if possible\")\n\n        # Modify plan based on time of day\n        if context.get('time_of_day') == 'night':\n            plan.constraints.append(\"Reduce movement speed for safety\")\n            plan.constraints.append(\"Use navigation lights if available\")\n\n        # Modify plan based on people presence\n        if context.get('people_present'):\n            plan.constraints.append(\"Maintain safe distance from humans\")\n            plan.constraints.append(\"Announce movements before executing\")\n\n        # Add social constraints\n        if context.get('social_context', {}).get('formal_setting', False):\n            plan.constraints.append(\"Use formal communication\")\n            plan.constraints.append(\"Maintain professional posture\")\n\n        return plan\n\n    def _simple_contextual_plan(self, enhanced_goal: str) -> ActionPlan:\n        \"\"\"Simple implementation for demonstration\"\"\"\n        # In a real implementation, this would call the LLM\n        # For this example, we'll return a basic plan\n        return ActionPlan(\n            id=\"contextual_plan_001\",\n            goal=enhanced_goal,\n            steps=[\n                {\n                    'id': 'analyze_context',\n                    'action': 'analyze',\n                    'parameters': {'context': enhanced_goal},\n                    'expected_outcome': 'Context analyzed and understood',\n                    'safety_check': 'none',\n                    'recovery_plan': 'ask_for_clarification'\n                },\n                {\n                    'id': 'execute_action',\n                    'action': 'execute',\n                    'parameters': {'goal': enhanced_goal},\n                    'expected_outcome': 'Goal achieved considering context',\n                    'safety_check': 'apply_contextual_safety',\n                    'recovery_plan': 'reassess_and_retry'\n                }\n            ],\n            constraints=[],\n            estimated_duration=10.0\n        )\n"})}),"\n",(0,o.jsx)(n.h2,{id:"mapping-symbolic-plans-to-ros-2-actions",children:"Mapping Symbolic Plans to ROS 2 Actions"}),"\n",(0,o.jsx)(n.h3,{id:"ros-2-action-integration",children:"ROS 2 Action Integration"}),"\n",(0,o.jsx)(n.p,{children:"Convert symbolic plans to executable ROS 2 actions:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"# Example: Mapping symbolic plans to ROS 2 actions\nimport rclpy\nfrom rclpy.action import ActionClient\nfrom rclpy.node import Node\nfrom geometry_msgs.msg import PoseStamped, Twist\nfrom nav_msgs.msg import Path\nfrom std_msgs.msg import String\nfrom move_base_msgs.action import MoveBase\nfrom sensor_msgs.msg import JointState\nfrom control_msgs.action import FollowJointTrajectory\n\nclass PlanToROS2Mapper(Node):\n    def __init__(self):\n        super().__init__('plan_to_ros2_mapper')\n\n        # Publishers\n        self.cmd_vel_pub = self.create_publisher(Twist, '/cmd_vel', 10)\n        self.goal_pub = self.create_publisher(PoseStamped, '/move_base_simple/goal', 10)\n        self.joint_cmd_pub = self.create_publisher(JointState, '/joint_commands', 10)\n\n        # Action clients\n        self.nav_client = ActionClient(self, MoveBase, 'move_base')\n        self.manipulation_client = ActionClient(self, FollowJointTrajectory, 'manipulator_controller/follow_joint_trajectory')\n\n        # Plan execution\n        self.plan_sub = self.create_subscription(\n            String,\n            '/action_plan',\n            self.plan_callback,\n            10\n        )\n\n        self.get_logger().info(\"Plan to ROS 2 Mapper initialized\")\n\n    def plan_callback(self, msg):\n        \"\"\"Receive action plan and execute\"\"\"\n        try:\n            # Parse plan from JSON string\n            plan_data = json.loads(msg.data)\n            plan = ActionPlan(**plan_data)\n\n            # Execute plan steps sequentially\n            self.execute_plan(plan)\n\n        except Exception as e:\n            self.get_logger().error(f\"Error executing plan: {e}\")\n\n    def execute_plan(self, plan: ActionPlan):\n        \"\"\"Execute action plan steps\"\"\"\n        for step in plan.steps:\n            success = self.execute_step(step)\n            if not success:\n                self.get_logger().error(f\"Step {step['id']} failed\")\n\n                # Execute recovery plan\n                recovery_success = self.execute_recovery(step.get('recovery_plan', ''))\n                if not recovery_success:\n                    self.get_logger().error(f\"Recovery plan failed for step {step['id']}\")\n                    break  # Stop execution if recovery fails\n\n    def execute_step(self, step: Dict) -> bool:\n        \"\"\"Execute individual step based on action type\"\"\"\n        action_type = step.get('action', 'unknown')\n\n        if action_type == 'navigate':\n            return self.execute_navigate_step(step)\n        elif action_type == 'move':\n            return self.execute_move_step(step)\n        elif action_type == 'manipulate':\n            return self.execute_manipulate_step(step)\n        elif action_type == 'perceive':\n            return self.execute_perceive_step(step)\n        elif action_type == 'communicate':\n            return self.execute_communicate_step(step)\n        else:\n            self.get_logger().warn(f\"Unknown action type: {action_type}\")\n            return False\n\n    def execute_navigate_step(self, step: Dict) -> bool:\n        \"\"\"Execute navigation step using MoveBase action\"\"\"\n        try:\n            # Wait for action server\n            if not self.nav_client.wait_for_server(timeout_sec=5.0):\n                self.get_logger().error(\"Navigation server not available\")\n                return False\n\n            # Extract destination\n            params = step.get('parameters', {})\n            destination = params.get('destination')\n\n            if not destination:\n                self.get_logger().error(\"No destination specified for navigation\")\n                return False\n\n            # Convert destination to PoseStamped\n            goal_pose = self._convert_destination_to_pose(destination)\n\n            # Create navigation goal\n            goal_msg = MoveBase.Goal()\n            goal_msg.target_pose = goal_pose\n\n            # Send goal\n            future = self.nav_client.send_goal_async(goal_msg)\n\n            # Wait for result\n            rclpy.spin_until_future_complete(self, future)\n\n            result = future.result()\n            return result.success if result else False\n\n        except Exception as e:\n            self.get_logger().error(f\"Navigation step execution failed: {e}\")\n            return False\n\n    def execute_move_step(self, step: Dict) -> bool:\n        \"\"\"Execute movement step using velocity commands\"\"\"\n        try:\n            params = step.get('parameters', {})\n\n            # Create velocity command\n            cmd_vel = Twist()\n\n            if 'direction' in params:\n                direction = params['direction']\n                speed = params.get('speed', 0.3)\n\n                if direction == 'forward':\n                    cmd_vel.linear.x = speed\n                elif direction == 'backward':\n                    cmd_vel.linear.x = -speed\n                elif direction == 'left':\n                    cmd_vel.linear.y = speed\n                elif direction == 'right':\n                    cmd_vel.linear.y = -speed\n                elif direction == 'up':\n                    cmd_vel.linear.z = speed\n                elif direction == 'down':\n                    cmd_vel.linear.z = -speed\n                elif direction == 'rotate_left':\n                    cmd_vel.angular.z = speed\n                elif direction == 'rotate_right':\n                    cmd_vel.angular.z = -speed\n\n            # Execute for specified duration\n            duration = params.get('duration', 1.0)\n            self._execute_velocity_command(cmd_vel, duration)\n\n            return True\n\n        except Exception as e:\n            self.get_logger().error(f\"Move step execution failed: {e}\")\n            return False\n\n    def execute_manipulate_step(self, step: Dict) -> bool:\n        \"\"\"Execute manipulation step using joint trajectory control\"\"\"\n        try:\n            # Wait for manipulation server\n            if not self.manipulation_client.wait_for_server(timeout_sec=5.0):\n                self.get_logger().error(\"Manipulation server not available\")\n                return False\n\n            params = step.get('parameters', {})\n            operation = params.get('operation', 'unknown')\n\n            if operation == 'grasp':\n                trajectory = self._create_grasp_trajectory(params)\n            elif operation == 'release':\n                trajectory = self._create_release_trajectory(params)\n            else:\n                self.get_logger().error(f\"Unsupported manipulation operation: {operation}\")\n                return False\n\n            # Create manipulation goal\n            goal_msg = FollowJointTrajectory.Goal()\n            goal_msg.trajectory = trajectory\n\n            # Send goal\n            future = self.manipulation_client.send_goal_async(goal_msg)\n\n            # Wait for result\n            rclpy.spin_until_future_complete(self, future)\n\n            result = future.result()\n            return result.success if result else False\n\n        except Exception as e:\n            self.get_logger().error(f\"Manipulation step execution failed: {e}\")\n            return False\n\n    def _execute_velocity_command(self, cmd_vel: Twist, duration: float):\n        \"\"\"Execute velocity command for specified duration\"\"\"\n        start_time = self.get_clock().now()\n        end_time = start_time + Duration(seconds=duration)\n\n        while self.get_clock().now() < end_time and rclpy.ok():\n            self.cmd_vel_pub.publish(cmd_vel)\n            self.get_clock().sleep_for(Duration(seconds=0.1))  # 10Hz\n\n    def _convert_destination_to_pose(self, destination: str) -> PoseStamped:\n        \"\"\"Convert destination string to PoseStamped\"\"\"\n        # In a real implementation, this would look up the destination\n        # in a map or use semantic navigation\n        pose = PoseStamped()\n        pose.header.frame_id = 'map'\n        pose.header.stamp = self.get_clock().now().to_msg()\n\n        # Placeholder coordinates - in reality would come from semantic map\n        if destination == 'kitchen':\n            pose.pose.position.x = 5.0\n            pose.pose.position.y = 3.0\n        elif destination == 'living_room':\n            pose.pose.position.x = 2.0\n            pose.pose.position.y = 1.0\n        else:\n            # Default to current position + offset\n            pose.pose.position.x = 1.0\n            pose.pose.position.y = 1.0\n\n        return pose\n\n    def execute_recovery(self, recovery_plan: str) -> bool:\n        \"\"\"Execute recovery plan\"\"\"\n        if recovery_plan == 'stop_and_replan':\n            # Stop current execution and signal for replanning\n            stop_cmd = Twist()\n            self.cmd_vel_pub.publish(stop_cmd)\n            return True\n        elif recovery_plan == 'abort_and_return_home':\n            # Navigate back to home position\n            return self._return_to_home()\n        else:\n            self.get_logger().info(f\"No specific recovery for: {recovery_plan}\")\n            return True  # Continue with best effort\n\n    def _return_to_home(self) -> bool:\n        \"\"\"Navigate back to home position\"\"\"\n        home_pose = PoseStamped()\n        home_pose.header.frame_id = 'map'\n        home_pose.pose.position.x = 0.0\n        home_pose.pose.position.y = 0.0\n        home_pose.pose.position.z = 0.0\n        home_pose.pose.orientation.w = 1.0\n\n        self.goal_pub.publish(home_pose)\n        return True\n\ndef main(args=None):\n    rclpy.init(args=args)\n\n    mapper = PlanToROS2Mapper()\n\n    try:\n        rclpy.spin(mapper)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        mapper.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,o.jsx)(n.h3,{id:"safety-validation-layer",children:"Safety Validation Layer"}),"\n",(0,o.jsx)(n.p,{children:"Add a safety validation layer to ensure LLM-generated plans are safe:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"# Example: Safety validation for LLM-generated plans\nclass PlanSafetyValidator:\n    def __init__(self):\n        self.safety_rules = [\n            self._validate_navigation_safety,\n            self._validate_manipulation_safety,\n            self._validate_human_interaction_safety,\n            self._validate_energy_consumption,\n            self._validate_physical_constraints\n        ]\n\n    def validate_plan(self, plan: ActionPlan) -> tuple[bool, List[str], ActionPlan]:\n        \"\"\"Validate plan for safety compliance\"\"\"\n        issues = []\n\n        # Apply all safety rules\n        for rule_func in self.safety_rules:\n            is_safe, rule_issues, modified_plan = rule_func(plan)\n            issues.extend(rule_issues)\n            plan = modified_plan\n\n            if not is_safe:\n                return False, issues, plan\n\n        return True, issues, plan\n\n    def _validate_navigation_safety(self, plan: ActionPlan) -> tuple[bool, List[str], ActionPlan]:\n        \"\"\"Validate navigation safety in plan\"\"\"\n        issues = []\n\n        for step in plan.steps:\n            if step.get('action') == 'navigate':\n                params = step.get('parameters', {})\n\n                # Check for safe destinations\n                dest = params.get('destination', '').lower()\n                if dest in ['roof', 'dangerous_area', 'restricted_zone']:\n                    issues.append(f\"Unsafe destination: {dest}\")\n\n                # Check navigation speed\n                speed = params.get('speed', 0)\n                if speed > 0.8:  # Too fast for humanoid\n                    issues.append(f\"Navigation speed too high: {speed} m/s\")\n                    # Modify plan to reduce speed\n                    step['parameters']['speed'] = 0.5\n\n        return len(issues) == 0, issues, plan\n\n    def _validate_manipulation_safety(self, plan: ActionPlan) -> tuple[bool, List[str], ActionPlan]:\n        \"\"\"Validate manipulation safety in plan\"\"\"\n        issues = []\n\n        for step in plan.steps:\n            if step.get('action') == 'manipulate':\n                params = step.get('parameters', {})\n\n                # Check force limits\n                force = params.get('force', 0)\n                if force > 50:  # N - too much for delicate manipulation\n                    issues.append(f\"Force too high for manipulation: {force}N\")\n\n                # Check object validity\n                target = params.get('target', '').lower()\n                if target in ['fragile_item', 'sharp_object']:\n                    step.setdefault('safety_checks', []).append('extra_caution_required')\n\n        return len(issues) == 0, issues, plan\n\n    def _validate_human_interaction_safety(self, plan: ActionPlan) -> tuple[bool, List[str], ActionPlan]:\n        \"\"\"Validate human interaction safety in plan\"\"\"\n        issues = []\n\n        for step in plan.steps:\n            action = step.get('action', '').lower()\n            params = step.get('parameters', {})\n\n            # Check for actions near humans\n            if 'approach' in action or 'move' in action:\n                distance = params.get('distance', 1.0)\n                if distance < 0.5:  # Too close to humans\n                    issues.append(f\"Unsafe distance to humans: {distance}m\")\n                    # Modify to maintain safe distance\n                    step['parameters']['distance'] = max(0.5, distance)\n\n        return len(issues) == 0, issues, plan\n\n    def _validate_energy_consumption(self, plan: ActionPlan) -> tuple[bool, List[str], ActionPlan]:\n        \"\"\"Validate energy consumption in plan\"\"\"\n        # Estimate energy consumption based on plan complexity\n        estimated_steps = len(plan.steps)\n\n        if estimated_steps > 50:  # Too many steps for single battery charge\n            plan.constraints.append(\"Energy conservation required: break task into segments\")\n\n        return True, [], plan\n\n    def _validate_physical_constraints(self, plan: ActionPlan) -> tuple[bool, List[str], ActionPlan]:\n        \"\"\"Validate physical constraints in plan\"\"\"\n        issues = []\n\n        for step in plan.steps:\n            params = step.get('parameters', {})\n\n            # Check height constraints\n            height = params.get('height', 0)\n            if height > 1.8:  # Above humanoid reach\n                issues.append(f\"Height too high: {height}m\")\n\n            # Check weight constraints\n            weight = params.get('weight', 0)\n            if weight > 5:  # 5kg too heavy for humanoid\n                issues.append(f\"Weight too heavy: {weight}kg\")\n\n        return len(issues) == 0, issues, plan\n"})}),"\n",(0,o.jsx)(n.h2,{id:"practical-exercise-complete-llm-based-planning-system",children:"Practical Exercise: Complete LLM-Based Planning System"}),"\n",(0,o.jsx)(n.p,{children:"Let's create a complete example that integrates all concepts:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# Complete example: LLM-based cognitive planning system\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Twist\nimport json\nimport openai\nfrom typing import Dict, Any\n\nclass CompleteLLMCognitiveSystem(Node):\n    def __init__(self):\n        super().__init__(\'complete_llm_cognitive_system\')\n\n        # Initialize components\n        self.planner = LLMBasedCognitivePlanner(api_key=self.get_parameter_or(\'openai_api_key\', \'\'))\n        self.grounding_layer = SafetyGroundingLayer()\n        self.plan_validator = PlanSafetyValidator()\n        self.ros_mapper = PlanToROS2Mapper(self)\n\n        # Publishers and subscribers\n        self.voice_command_sub = self.create_subscription(\n            String,\n            \'/voice_commands\',\n            self.voice_command_callback,\n            10\n        )\n\n        self.plan_pub = self.create_publisher(String, \'/action_plan\', 10)\n        self.status_pub = self.create_publisher(String, \'/cognitive_status\', 10)\n\n        # Context manager\n        self.context_manager = ContextAwarePlanner()\n\n        self.get_logger().info("Complete LLM Cognitive System initialized")\n\n    def voice_command_callback(self, msg):\n        """Process voice command through full cognitive pipeline"""\n        try:\n            self.get_logger().info(f"Processing voice command: {msg.data}")\n\n            # 1. Parse natural language goal\n            parsed_goal = self.parse_natural_language_goal(msg.data)\n\n            # 2. Get current context\n            current_context = self.get_current_context()\n\n            # 3. Create contextual plan\n            contextual_plan = self.context_manager.plan_with_context(msg.data, current_context)\n\n            # 4. Validate plan safety\n            is_safe, issues, validated_plan = self.plan_validator.validate_plan(contextual_plan)\n\n            if not is_safe:\n                self.get_logger().error(f"Plan validation failed: {issues}")\n                self.publish_status(f"Plan rejected: {\', \'.join(issues)}")\n                return\n\n            # 5. Apply safety grounding\n            is_groundable, grounding_msg, grounded_plan = self.grounding_layer.ground_llm_output(validated_plan.__dict__)\n\n            if not is_groundable:\n                self.get_logger().error(f"Plan grounding failed: {grounding_msg}")\n                self.publish_status(f"Plan grounded: {grounding_msg}")\n                return\n\n            # 6. Publish validated action plan\n            plan_json = json.dumps(grounded_plan)\n            plan_msg = String()\n            plan_msg.data = plan_json\n            self.plan_pub.publish(plan_msg)\n\n            self.get_logger().info(f"Published action plan for execution: {len(grounded_plan.get(\'steps\', []))} steps")\n            self.publish_status(f"Plan executed: {len(grounded_plan.get(\'steps\', []))} steps")\n\n        except Exception as e:\n            self.get_logger().error(f"Error processing voice command: {e}")\n            self.publish_status(f"Error: {str(e)}")\n\n    def parse_natural_language_goal(self, text: str) -> Dict[str, Any]:\n        """Parse natural language goal using NL processor"""\n        nl_processor = NaturalLanguageToAction()\n        return nl_processor.parse_goal(text)\n\n    def get_current_context(self) -> Dict:\n        """Get current robot and environment context"""\n        # In a real implementation, this would gather context from various sensors and systems\n        return {\n            \'location\': \'current_room\',\n            \'time_of_day\': \'daytime\',\n            \'people_present\': [],\n            \'battery_level\': 85,\n            \'current_pose\': {\'x\': 0.0, \'y\': 0.0, \'theta\': 0.0},\n            \'social_context\': {\'formal_setting\': False}\n        }\n\n    def publish_status(self, status: str):\n        """Publish cognitive system status"""\n        status_msg = String()\n        status_msg.data = status\n        self.status_pub.publish(status_msg)\n\ndef main(args=None):\n    rclpy.init(args=args)\n\n    cognitive_system = CompleteLLMCognitiveSystem()\n\n    try:\n        rclpy.spin(cognitive_system)\n    except KeyboardInterrupt:\n        cognitive_system.get_logger().info("Shutting down cognitive system...")\n    finally:\n        cognitive_system.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,o.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,o.jsx)(n.p,{children:"In this chapter, you've learned how to:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Integrate large language models with humanoid robot cognitive systems"}),"\n",(0,o.jsx)(n.li,{children:"Translate natural language goals into executable action plans"}),"\n",(0,o.jsx)(n.li,{children:"Apply safety and grounding constraints to LLM outputs"}),"\n",(0,o.jsx)(n.li,{children:"Map symbolic plans to ROS 2 actions for robot execution"}),"\n",(0,o.jsx)(n.li,{children:"Implement context-aware planning for adaptive behavior"}),"\n",(0,o.jsx)(n.li,{children:"Validate plans for safety and feasibility"}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:["In the ",(0,o.jsx)(n.a,{href:"./03-autonomous-humanoid",children:"next chapter"}),", we'll explore how to build a complete end-to-end system that coordinates all VLA components with ROS 2 for a fully autonomous humanoid robot capable of responding to voice commands with intelligent actions."]})]})}function d(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(p,{...e})}):p(e)}},8453(e,n,t){t.d(n,{R:()=>i,x:()=>r});var a=t(6540);const o={},s=a.createContext(o);function i(e){const n=a.useContext(s);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:i(e.components),a.createElement(s.Provider,{value:n},e.children)}}}]);