"use strict";(self.webpackChunkmybookproject=self.webpackChunkmybookproject||[]).push([[9466],{6230(e,n,a){a.r(n),a.d(n,{assets:()=>l,contentTitle:()=>t,default:()=>d,frontMatter:()=>o,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"module-3/isaac-ros-perception-navigation","title":"Module 3, Chapter 2: Isaac ROS for Perception and Navigation","description":"Learn to implement perception and navigation using Isaac ROS nodes and pipelines with hardware-accelerated VSLAM","source":"@site/docs/module-3/02-isaac-ros-perception-navigation.md","sourceDirName":"module-3","slug":"/module-3/isaac-ros-perception-navigation","permalink":"/docs/module-3/isaac-ros-perception-navigation","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2,"title":"Module 3, Chapter 2: Isaac ROS for Perception and Navigation","description":"Learn to implement perception and navigation using Isaac ROS nodes and pipelines with hardware-accelerated VSLAM"},"sidebar":"tutorialSidebar","previous":{"title":"Module 3, Chapter 1: NVIDIA Isaac Sim Overview","permalink":"/docs/module-3/isaac-sim-overview"},"next":{"title":"Module 3, Chapter 3: Path Planning with Nav2","permalink":"/docs/module-3/nav2-path-planning"}}');var s=a(4848),r=a(8453);const o={sidebar_position:2,title:"Module 3, Chapter 2: Isaac ROS for Perception and Navigation",description:"Learn to implement perception and navigation using Isaac ROS nodes and pipelines with hardware-accelerated VSLAM"},t="Module 3, Chapter 2: Isaac ROS for Perception and Navigation",l={},c=[{value:"Overview",id:"overview",level:2},{value:"Isaac ROS Nodes and Pipelines",id:"isaac-ros-nodes-and-pipelines",level:2},{value:"Introduction to Isaac ROS",id:"introduction-to-isaac-ros",level:3},{value:"Core Isaac ROS Packages",id:"core-isaac-ros-packages",level:3},{value:"Basic Pipeline Architecture",id:"basic-pipeline-architecture",level:3},{value:"Hardware-Accelerated Visual SLAM (VSLAM)",id:"hardware-accelerated-visual-slam-vslam",level:2},{value:"Understanding Visual SLAM",id:"understanding-visual-slam",level:3},{value:"Isaac ROS Visual SLAM Components",id:"isaac-ros-visual-slam-components",level:3},{value:"Launching Isaac ROS VSLAM",id:"launching-isaac-ros-vslam",level:3},{value:"VSLAM Performance Optimization",id:"vslam-performance-optimization",level:3},{value:"Mapping and Localization Concepts",id:"mapping-and-localization-concepts",level:2},{value:"Occupancy Grid Mapping",id:"occupancy-grid-mapping",level:3},{value:"Localization with Isaac ROS",id:"localization-with-isaac-ros",level:3},{value:"Sensor Integration with ROS 2",id:"sensor-integration-with-ros-2",level:2},{value:"Isaac ROS Sensor Processing Pipeline",id:"isaac-ros-sensor-processing-pipeline",level:3},{value:"Synchronization and Timing",id:"synchronization-and-timing",level:3},{value:"Practical Exercise: Complete Perception and Navigation Pipeline",id:"practical-exercise-complete-perception-and-navigation-pipeline",level:2},{value:"Summary",id:"summary",level:2}];function p(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"module-3-chapter-2-isaac-ros-for-perception-and-navigation",children:"Module 3, Chapter 2: Isaac ROS for Perception and Navigation"})}),"\n",(0,s.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,s.jsx)(n.p,{children:"Isaac ROS brings NVIDIA's hardware-accelerated perception and navigation capabilities to the ROS 2 ecosystem. This chapter covers how to implement perception and navigation using Isaac ROS nodes and pipelines with hardware-accelerated Visual SLAM (VSLAM) and sensor integration."}),"\n",(0,s.jsx)(n.h2,{id:"isaac-ros-nodes-and-pipelines",children:"Isaac ROS Nodes and Pipelines"}),"\n",(0,s.jsx)(n.h3,{id:"introduction-to-isaac-ros",children:"Introduction to Isaac ROS"}),"\n",(0,s.jsx)(n.p,{children:"Isaac ROS is a collection of hardware-accelerated perception and navigation packages designed for robotics applications. Key advantages include:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"GPU Acceleration"}),": Leveraging NVIDIA GPUs for real-time processing"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Deep Learning Integration"}),": Optimized for AI-based perception"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Modular Architecture"}),": Flexible pipeline construction"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"ROS 2 Native"}),": Seamless integration with ROS 2 ecosystem"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"core-isaac-ros-packages",children:"Core Isaac ROS Packages"}),"\n",(0,s.jsx)(n.p,{children:"The Isaac ROS ecosystem includes several key packages:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Isaac ROS Visual SLAM"}),": Hardware-accelerated simultaneous localization and mapping"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Isaac ROS Stereo Dense Reconstruction"}),": 3D reconstruction from stereo cameras"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Isaac ROS AprilTag"}),": Marker-based pose estimation"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Isaac ROS CenterPose"}),": 6D object pose estimation"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Isaac ROS DNN Image Encoding"}),": GPU-accelerated neural network inference"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Isaac ROS Image Pipeline"}),": Image processing and rectification"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"basic-pipeline-architecture",children:"Basic Pipeline Architecture"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# Example: Basic Isaac ROS pipeline\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, CameraInfo\nfrom geometry_msgs.msg import PoseStamped\nfrom nav_msgs.msg import Odometry\nfrom visualization_msgs.msg import MarkerArray\n\nclass IsaacROSPipeline(Node):\n    def __init__(self):\n        super().__init__('isaac_ros_pipeline')\n\n        # Subscribers for sensor data\n        self.rgb_sub = self.create_subscription(\n            Image,\n            '/camera/rgb/image_rect_color',\n            self.rgb_callback,\n            10\n        )\n\n        self.depth_sub = self.create_subscription(\n            Image,\n            '/camera/depth/image_rect_raw',\n            self.depth_callback,\n            10\n        )\n\n        self.camera_info_sub = self.create_subscription(\n            CameraInfo,\n            '/camera/rgb/camera_info',\n            self.camera_info_callback,\n            10\n        )\n\n        # Publishers for processed data\n        self.odom_pub = self.create_publisher(Odometry, '/odom', 10)\n        self.pose_pub = self.create_publisher(PoseStamped, '/pose', 10)\n        self.marker_pub = self.create_publisher(MarkerArray, '/landmarks', 10)\n\n        # Processing pipeline components\n        self.vslam_processor = VSLAMProcessor()\n        self.perception_pipeline = PerceptionPipeline()\n\n    def rgb_callback(self, msg):\n        # Process RGB image through perception pipeline\n        features = self.perception_pipeline.extract_features(msg)\n        self.vslam_processor.update_rgb(features, msg.header.stamp)\n\n    def depth_callback(self, msg):\n        # Process depth image\n        point_cloud = self.perception_pipeline.reconstruct_3d(msg)\n        self.vslam_processor.update_depth(point_cloud, msg.header.stamp)\n\n    def camera_info_callback(self, msg):\n        # Update camera parameters\n        self.vslam_processor.update_camera_intrinsics(msg)\n"})}),"\n",(0,s.jsx)(n.h2,{id:"hardware-accelerated-visual-slam-vslam",children:"Hardware-Accelerated Visual SLAM (VSLAM)"}),"\n",(0,s.jsx)(n.h3,{id:"understanding-visual-slam",children:"Understanding Visual SLAM"}),"\n",(0,s.jsx)(n.p,{children:"Visual SLAM (Simultaneous Localization and Mapping) uses visual sensors to estimate the camera's position and orientation while simultaneously mapping the environment. Isaac ROS provides hardware-accelerated VSLAM capabilities:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Real-time Performance"}),": GPU acceleration enables real-time operation"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"High Accuracy"}),": Visual-inertial fusion for improved tracking"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Robust Tracking"}),": Advanced feature detection and matching"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Loop Closure"}),": Detection and correction of accumulated errors"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"isaac-ros-visual-slam-components",children:"Isaac ROS Visual SLAM Components"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# Example: Isaac ROS Visual SLAM setup\nfrom isaac_ros_visual_slam import VisualSLAMNode\n\nclass VisualSLAMManager(Node):\n    def __init__(self):\n        super().__init__('visual_slam_manager')\n\n        # Isaac ROS VSLAM node\n        self.vslam_node = VisualSLAMNode(\n            node=self,\n            input_image_topic='/camera/rgb/image_rect_color',\n            input_camera_info_topic='/camera/rgb/camera_info',\n            input_imu_topic='/imu/data',\n            output_tracking_topic='/nvblox_traversable/map',\n            output_map_topic='/map',\n            output_odom_topic='/visual_slam/odometry'\n        )\n\n        # Configure VSLAM parameters\n        self.configure_vslam_params()\n\n    def configure_vslam_params(self):\n        # Set VSLAM parameters for humanoid navigation\n        self.set_parameters([\n            Parameter('enable_debug_mode', Parameter.Type.BOOL, False),\n            Parameter('enable_eskf', Parameter.Type.BOOL, True),  # Enable ESKF\n            Parameter('input_rate', Parameter.Type.DOUBLE, 30.0),  # 30 Hz\n            Parameter('num_keyframes', Parameter.Type.INTEGER, 20),  # Max keyframes\n            Parameter('min_num_landmarks', Parameter.Type.INTEGER, 50),  # Min landmarks\n        ])\n"})}),"\n",(0,s.jsx)(n.h3,{id:"launching-isaac-ros-vslam",children:"Launching Isaac ROS VSLAM"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-xml",children:'\x3c!-- Example: Isaac ROS VSLAM launch file --\x3e\n<launch>\n  \x3c!-- Visual SLAM node --\x3e\n  <node pkg="isaac_ros_visual_slam" exec="visual_slam_node" name="visual_slam">\n    <param name="enable_debug_mode" value="false"/>\n    <param name="enable_eskf" value="true"/>\n    <param name="input_rate" value="30.0"/>\n    <param name="publish_traverse_distance" value="true"/>\n    <param name="publish_point_cloud" value="true"/>\n    <param name="publish_local_map" value="true"/>\n    <param name="publish_separate_thread" value="true"/>\n    <param name="input_reliable_qos" value="2"/>\n    <param name="map_frame" value="map"/>\n    <param name="odom_frame" value="odom"/>\n    <param name="base_frame" value="base_link"/>\n    <param name="enable_occupancy_map" value="true"/>\n  </node>\n\n  \x3c!-- Image rectification for stereo cameras --\x3e\n  <node pkg="isaac_ros_stereo_image_proc" exec="rectify_node" name="left_rectify_node">\n    <param name="input_camera_info_topic" value="/camera/left/camera_info"/>\n    <param name="input_image_topic" value="/camera/left/image_raw"/>\n    <param name="output_camera_info_topic" value="/camera/left/camera_info_rect"/>\n    <param name="output_image_topic" value="/camera/left/image_rect"/>\n  </node>\n\n  <node pkg="isaac_ros_stereo_image_proc" exec="rectify_node" name="right_rectify_node">\n    <param name="input_camera_info_topic" value="/camera/right/camera_info"/>\n    <param name="input_image_topic" value="/camera/right/image_raw"/>\n    <param name="output_camera_info_topic" value="/camera/right/camera_info_rect"/>\n    <param name="output_image_topic" value="/camera/right/image_rect"/>\n  </node>\n</launch>\n'})}),"\n",(0,s.jsx)(n.h3,{id:"vslam-performance-optimization",children:"VSLAM Performance Optimization"}),"\n",(0,s.jsx)(n.p,{children:"For humanoid robots, consider these optimization strategies:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# Example: VSLAM optimization for humanoid movement\nclass OptimizedVSLAM(VisualSLAMNode):\n    def __init__(self):\n        super().__init__()\n\n        # Adaptive tracking for humanoid movement patterns\n        self.humanoid_adaptive_params = {\n            'translation_threshold': 0.05,  # 5cm threshold for keyframe insertion\n            'rotation_threshold': 0.1,      # 0.1 rad threshold for keyframe insertion\n            'min_feature_count': 50,        # Minimum features to maintain tracking\n            'max_feature_count': 1000,      # Maximum features to avoid overload\n            'tracking_timeout': 1.0         # Timeout for tracking recovery\n        }\n\n        # Dynamic parameter adjustment based on movement\n        self.movement_state = 'stationary'  # stationary, walking, turning, running\n        self.last_pose = None\n        self.velocity_estimator = VelocityEstimator()\n\n    def update_tracking_strategy(self, current_pose):\n        if self.last_pose is not None:\n            velocity = self.velocity_estimator.calculate_velocity(\n                self.last_pose, current_pose, self.get_clock().now()\n            )\n\n            # Adjust tracking parameters based on movement\n            if velocity.linear < 0.1:  # Stationary\n                self.set_tracking_params(self.humanoid_adaptive_params['stationary'])\n            elif velocity.linear < 0.5:  # Walking\n                self.set_tracking_params(self.humanoid_adaptive_params['walking'])\n            elif velocity.linear < 1.0:  # Turning\n                self.set_tracking_params(self.humanoid_adaptive_params['turning'])\n            else:  # Fast movement\n                self.set_tracking_params(self.humanoid_adaptive_params['fast'])\n\n        self.last_pose = current_pose\n"})}),"\n",(0,s.jsx)(n.h2,{id:"mapping-and-localization-concepts",children:"Mapping and Localization Concepts"}),"\n",(0,s.jsx)(n.h3,{id:"occupancy-grid-mapping",children:"Occupancy Grid Mapping"}),"\n",(0,s.jsx)(n.p,{children:"Isaac ROS provides occupancy grid mapping capabilities for navigation:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# Example: Isaac ROS occupancy mapping\nfrom nvblox_msgs.msg import NvbloxMap\nfrom geometry_msgs.msg import Point\nimport numpy as np\n\nclass OccupancyMapper:\n    def __init__(self, node):\n        self.node = node\n\n        # Subscriber for traversable map\n        self.map_sub = self.node.create_subscription(\n            NvbloxMap,\n            '/nvblox_traversable/map',\n            self.map_callback,\n            10\n        )\n\n        # Publisher for occupancy grid\n        self.grid_pub = self.node.create_publisher(\n            OccupancyGrid,\n            '/local_costmap/costmap',\n            10\n        )\n\n    def map_callback(self, msg):\n        # Convert Nvblox map to occupancy grid\n        occupancy_grid = self.convert_to_occupancy_grid(msg)\n\n        # Publish for navigation stack\n        self.grid_pub.publish(occupancy_grid)\n\n    def convert_to_occupancy_grid(self, nvblox_map):\n        # Process the 3D map to 2D occupancy grid\n        grid_data = np.zeros((msg.height, msg.width), dtype=np.int8)\n\n        # Extract traversable regions\n        for point in nvblox_map.points:\n            x_idx = int((point.x - msg.origin.position.x) / msg.resolution)\n            y_idx = int((point.y - msg.origin.position.y) / msg.resolution)\n\n            if 0 <= x_idx < msg.width and 0 <= y_idx < msg.height:\n                # Mark as traversable (0) or occupied (100)\n                grid_data[y_idx, x_idx] = 0 if point.traversable else 100\n\n        # Create OccupancyGrid message\n        occupancy_grid = OccupancyGrid()\n        occupancy_grid.header = nvblox_map.header\n        occupancy_grid.info.resolution = msg.resolution\n        occupancy_grid.info.width = msg.width\n        occupancy_grid.info.height = msg.height\n        occupancy_grid.info.origin = msg.origin\n        occupancy_grid.data = grid_data.flatten().tolist()\n\n        return occupancy_grid\n"})}),"\n",(0,s.jsx)(n.h3,{id:"localization-with-isaac-ros",children:"Localization with Isaac ROS"}),"\n",(0,s.jsx)(n.p,{children:"Accurate localization is crucial for humanoid navigation:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# Example: Isaac ROS localization integration\nfrom tf2_ros import TransformBroadcaster\nfrom geometry_msgs.msg import TransformStamped\nfrom tf_transformations import quaternion_from_euler\n\nclass LocalizationManager:\n    def __init__(self, node):\n        self.node = node\n        self.tf_broadcaster = TransformBroadcaster(self.node)\n\n        # Subscribers for different pose estimates\n        self.vslam_sub = self.node.create_subscription(\n            Odometry, '/visual_slam/odometry', self.vslam_callback, 10\n        )\n\n        self.imu_sub = self.node.create_subscription(\n            Imu, '/imu/data', self.imu_callback, 10\n        )\n\n        # Publisher for combined pose estimate\n        self.pose_pub = self.node.create_publisher(PoseWithCovarianceStamped, '/initialpose', 10)\n        self.amcl_pose_pub = self.node.create_publisher(PoseWithCovarianceStamped, '/amcl_pose', 10)\n\n    def vslam_callback(self, msg):\n        # Process VSLAM pose estimate\n        self.current_pose = msg.pose.pose\n        self.current_covariance = msg.pose.covariance\n\n        # Broadcast transform\n        t = TransformStamped()\n        t.header.stamp = self.node.get_clock().now().to_msg()\n        t.header.frame_id = 'map'\n        t.child_frame_id = 'odom'\n        t.transform.translation.x = self.current_pose.position.x\n        t.transform.translation.y = self.current_pose.position.y\n        t.transform.translation.z = self.current_pose.position.z\n        t.transform.rotation = self.current_pose.orientation\n\n        self.tf_broadcaster.sendTransform(t)\n\n    def fuse_poses(self):\n        # Fuse VSLAM, IMU, and other sensor data for robust localization\n        fused_pose = self.ekf_fusion(\n            vslam_pose=self.current_pose,\n            imu_data=self.last_imu,\n            wheel_odom=self.wheel_odom\n        )\n\n        return fused_pose\n"})}),"\n",(0,s.jsx)(n.h2,{id:"sensor-integration-with-ros-2",children:"Sensor Integration with ROS 2"}),"\n",(0,s.jsx)(n.h3,{id:"isaac-ros-sensor-processing-pipeline",children:"Isaac ROS Sensor Processing Pipeline"}),"\n",(0,s.jsx)(n.p,{children:"Integrating Isaac Sim sensors with ROS 2 requires careful consideration of data types and formats:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# Example: Isaac ROS sensor integration\nimport cv2\nimport numpy as np\nfrom sensor_msgs.msg import Image, CameraInfo, PointCloud2, PointField\nfrom cv_bridge import CvBridge\nfrom builtin_interfaces.msg import Time\n\nclass IsaacROSSensorIntegration:\n    def __init__(self, node):\n        self.node = node\n        self.bridge = CvBridge()\n\n        # Isaac Sim to ROS 2 publishers\n        self.rgb_pub = self.node.create_publisher(Image, '/camera/rgb/image_raw', 10)\n        self.depth_pub = self.node.create_publisher(Image, '/camera/depth/image_raw', 10)\n        self.camera_info_pub = self.node.create_publisher(CameraInfo, '/camera/rgb/camera_info', 10)\n        self.pointcloud_pub = self.node.create_publisher(PointCloud2, '/camera/depth/color/points', 10)\n\n        # Isaac Sim callbacks\n        self.setup_isaac_callbacks()\n\n    def setup_isaac_callbacks(self):\n        # Set up Isaac Sim callbacks to publish ROS 2 messages\n        # This would connect to Isaac Sim's rendering callbacks\n        pass\n\n    def convert_rgb_to_ros(self, rgb_image_data):\n        # Convert Isaac Sim RGB data to ROS Image message\n        ros_image = self.bridge.cv2_to_imgmsg(rgb_image_data, encoding='rgba8')\n        ros_image.header.stamp = self.node.get_clock().now().to_msg()\n        ros_image.header.frame_id = 'camera_rgb_optical_frame'\n        return ros_image\n\n    def convert_depth_to_ros(self, depth_data):\n        # Convert Isaac Sim depth data to ROS Image message\n        # Depth data typically comes as float32 array\n        depth_image = np.array(depth_data, dtype=np.float32)\n        ros_depth = self.bridge.cv2_to_imgmsg(depth_image, encoding='32FC1')\n        ros_depth.header.stamp = self.node.get_clock().now().to_msg()\n        ros_depth.header.frame_id = 'camera_depth_optical_frame'\n        return ros_depth\n\n    def create_pointcloud(self, depth_image, rgb_image, camera_info):\n        # Create PointCloud2 from depth and RGB images\n        height, width = depth_image.shape[:2]\n\n        # Get camera intrinsics\n        fx = camera_info.k[0]  # Camera matrix fx\n        fy = camera_info.k[4]  # Camera matrix fy\n        cx = camera_info.k[2]  # Camera matrix cx\n        cy = camera_info.k[5]  # Camera matrix cy\n\n        # Generate point cloud\n        points = []\n        for v in range(height):\n            for u in range(width):\n                z = depth_image[v, u]\n                if z > 0:  # Valid depth\n                    x = (u - cx) * z / fx\n                    y = (v - cy) * z / fy\n\n                    # Get RGB color\n                    if len(rgb_image.shape) == 3:\n                        r, g, b = rgb_image[v, u][:3]\n                        rgb = (int(r) << 16) | (int(g) << 8) | int(b)\n                    else:\n                        rgb = 0  # Default color\n\n                    points.append([x, y, z, rgb])\n\n        # Create PointCloud2 message\n        fields = [\n            PointField(name='x', offset=0, datatype=PointField.FLOAT32, count=1),\n            PointField(name='y', offset=4, datatype=PointField.FLOAT32, count=1),\n            PointField(name='z', offset=8, datatype=PointField.FLOAT32, count=1),\n            PointField(name='rgb', offset=12, datatype=PointField.UINT32, count=1)\n        ]\n\n        # Convert points to binary data\n        pointcloud_msg = PointCloud2()\n        pointcloud_msg.header.stamp = self.node.get_clock().now().to_msg()\n        pointcloud_msg.header.frame_id = 'camera_depth_optical_frame'\n        pointcloud_msg.height = 1\n        pointcloud_msg.width = len(points)\n        pointcloud_msg.fields = fields\n        pointcloud_msg.is_bigendian = False\n        pointcloud_msg.point_step = 16  # 3*4 bytes for xyz + 4 bytes for rgb\n        pointcloud_msg.row_step = pointcloud_msg.point_step * pointcloud_msg.width\n        pointcloud_msg.is_dense = True\n\n        # Pack the data\n        import struct\n        data = []\n        for point in points:\n            data.extend(struct.pack('fffI', point[0], point[1], point[2], int(point[3])))\n\n        pointcloud_msg.data = b''.join(data)\n        return pointcloud_msg\n"})}),"\n",(0,s.jsx)(n.h3,{id:"synchronization-and-timing",children:"Synchronization and Timing"}),"\n",(0,s.jsx)(n.p,{children:"Proper synchronization is critical for perception systems:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# Example: Isaac ROS sensor synchronization\nfrom rclpy.qos import QoSProfile, QoSHistoryPolicy, QoSReliabilityPolicy\nfrom message_filters import ApproximateTimeSynchronizer, Subscriber\nimport threading\n\nclass SensorSynchronizer:\n    def __init__(self, node):\n        self.node = node\n        self.sync_lock = threading.Lock()\n\n        # Define QoS profiles for different sensor types\n        reliable_qos = QoSProfile(\n            history=QoSHistoryPolicy.KEEP_LAST,\n            depth=10,\n            reliability=QoSReliabilityPolicy.RELIABLE\n        )\n\n        # Subscribers with appropriate QoS\n        self.rgb_sub = Subscriber(node, Image, '/camera/rgb/image_rect_color', qos_profile=reliable_qos)\n        self.depth_sub = Subscriber(node, Image, '/camera/depth/image_rect_raw', qos_profile=reliable_qos)\n        self.imu_sub = Subscriber(node, Imu, '/imu/data', qos_profile=reliable_qos)\n        self.odom_sub = Subscriber(node, Odometry, '/odom', qos_profile=reliable_qos)\n\n        # Synchronizer for RGB-D data\n        self.ts = ApproximateTimeSynchronizer(\n            [self.rgb_sub, self.depth_sub],\n            queue_size=10,\n            slop=0.1  # 100ms tolerance\n        )\n        self.ts.registerCallback(self.rgb_depth_callback)\n\n        # Separate IMU and odometry processing\n        self.imu_sub.registerCallback(self.imu_callback)\n        self.odom_sub.registerCallback(self.odom_callback)\n\n    def rgb_depth_callback(self, rgb_msg, depth_msg):\n        # Process synchronized RGB-D data\n        with self.sync_lock:\n            # Ensure timestamps are close enough\n            time_diff = abs(\n                self.node.get_clock().from_msg(rgb_msg.header.stamp).nanoseconds -\n                self.node.get_clock().from_msg(depth_msg.header.stamp).nanoseconds\n            )\n\n            if time_diff < 50000000:  # 50ms threshold\n                # Process synchronized pair\n                self.process_rgbd_pair(rgb_msg, depth_msg)\n            else:\n                self.node.get_logger().warn(f'Timestamp difference too large: {time_diff/1e6}ms')\n\n    def process_rgbd_pair(self, rgb_msg, depth_msg):\n        # Process the synchronized RGB-D data for perception\n        rgb_cv = self.bridge.imgmsg_to_cv2(rgb_msg, desired_encoding='bgr8')\n        depth_cv = self.bridge.imgmsg_to_cv2(depth_msg, desired_encoding='32FC1')\n\n        # Extract features and update perception pipeline\n        features = self.extract_features(rgb_cv, depth_cv)\n        self.update_perception_pipeline(features)\n"})}),"\n",(0,s.jsx)(n.h2,{id:"practical-exercise-complete-perception-and-navigation-pipeline",children:"Practical Exercise: Complete Perception and Navigation Pipeline"}),"\n",(0,s.jsx)(n.p,{children:"Let's create a complete example that integrates all concepts:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# Complete example: Isaac ROS perception and navigation pipeline\nimport rclpy\nfrom rclpy.node import Node\nfrom geometry_msgs.msg import Twist, PoseStamped\nfrom sensor_msgs.msg import Image, CameraInfo, Imu\nfrom nav_msgs.msg import Odometry\nfrom tf2_ros import TransformListener, Buffer\nfrom visualization_msgs.msg import MarkerArray\nimport numpy as np\n\nclass IsaacROSNavigationPipeline(Node):\n    def __init__(self):\n        super().__init__('isaac_ros_navigation_pipeline')\n\n        # Initialize perception components\n        self.vslam = VisualSLAMProcessor(self)\n        self.perception = PerceptionProcessor(self)\n        self.localization = LocalizationManager(self)\n        self.navigation = NavigationPlanner(self)\n\n        # TF buffer for coordinate transformations\n        self.tf_buffer = Buffer()\n        self.tf_listener = TransformListener(self.tf_buffer, self)\n\n        # Publishers and subscribers\n        self.cmd_vel_pub = self.create_publisher(Twist, '/cmd_vel', 10)\n        self.goal_pub = self.create_publisher(PoseStamped, '/move_base_simple/goal', 10)\n        self.marker_pub = self.create_publisher(MarkerArray, '/perception/markers', 10)\n\n        # Timer for main navigation loop\n        self.nav_timer = self.create_timer(0.1, self.navigation_loop)  # 10Hz\n\n        # Navigation state\n        self.current_goal = None\n        self.navigation_active = False\n\n        self.get_logger().info(\"Isaac ROS Navigation Pipeline initialized\")\n\n    def navigation_loop(self):\n        # Main navigation loop\n        if not self.navigation_active:\n            return\n\n        # Get current robot pose\n        try:\n            current_pose = self.tf_buffer.lookup_transform(\n                'map', 'base_link', rclpy.time.Time()\n            )\n        except Exception as e:\n            self.get_logger().warn(f\"Could not get robot pose: {e}\")\n            return\n\n        # Get perception data\n        obstacles = self.perception.get_obstacles()\n        landmarks = self.perception.get_landmarks()\n\n        # Plan path to goal\n        if self.current_goal:\n            path = self.navigation.plan_path(current_pose, self.current_goal, obstacles)\n\n            # Execute navigation\n            cmd_vel = self.navigation.compute_velocity_command(path, current_pose)\n            self.cmd_vel_pub.publish(cmd_vel)\n\n            # Check if goal reached\n            distance_to_goal = self.navigation.distance_to_pose(\n                current_pose, self.current_goal\n            )\n\n            if distance_to_goal < 0.5:  # 50cm tolerance\n                self.get_logger().info(\"Goal reached!\")\n                self.navigation_active = False\n                self.current_goal = None\n\n    def set_navigation_goal(self, goal_pose):\n        # Set a new navigation goal\n        self.current_goal = goal_pose\n        self.navigation_active = True\n        self.get_logger().info(f\"Navigation goal set: {goal_pose.pose.position}\")\n\ndef main(args=None):\n    rclpy.init(args=args)\n\n    node = IsaacROSNavigationPipeline()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,s.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,s.jsx)(n.p,{children:"In this chapter, you've learned how to:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Use Isaac ROS nodes and pipelines for perception and navigation"}),"\n",(0,s.jsx)(n.li,{children:"Implement hardware-accelerated Visual SLAM for mapping and localization"}),"\n",(0,s.jsx)(n.li,{children:"Integrate Isaac Sim sensors with ROS 2 using proper data formats"}),"\n",(0,s.jsx)(n.li,{children:"Apply mapping and localization concepts to humanoid robot navigation"}),"\n",(0,s.jsx)(n.li,{children:"Create complete perception and navigation pipelines"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["In the ",(0,s.jsx)(n.a,{href:"./03-nav2-path-planning",children:"next chapter"}),", we'll explore how to use Nav2 for path planning specifically adapted for bipedal humanoid movement patterns and constraints."]})]})}function d(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(p,{...e})}):p(e)}},8453(e,n,a){a.d(n,{R:()=>o,x:()=>t});var i=a(6540);const s={},r=i.createContext(s);function o(e){const n=i.useContext(r);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function t(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),i.createElement(r.Provider,{value:n},e.children)}}}]);