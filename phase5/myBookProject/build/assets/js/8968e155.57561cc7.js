"use strict";(self.webpackChunkmybookproject=self.webpackChunkmybookproject||[]).push([[2732],{6372(n,e,i){i.r(e),i.d(e,{assets:()=>l,contentTitle:()=>o,default:()=>g,frontMatter:()=>a,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"module-2/sensor-environment-integration","title":"Chapter 3: Sensor and Environment Integration","description":"Learn to connect simulated sensors to ROS 2 nodes and create complete digital twin systems","source":"@site/docs/module-2/03-sensor-environment-integration.md","sourceDirName":"module-2","slug":"/module-2/sensor-environment-integration","permalink":"/docs/module-2/sensor-environment-integration","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3,"title":"Chapter 3: Sensor and Environment Integration","description":"Learn to connect simulated sensors to ROS 2 nodes and create complete digital twin systems"},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 2: High-Fidelity Rendering in Unity","permalink":"/docs/module-2/high-fidelity-rendering-unity"},"next":{"title":"Module 3, Chapter 1: NVIDIA Isaac Sim Overview","permalink":"/docs/module-3/isaac-sim-overview"}}');var t=i(4848),r=i(8453);const a={sidebar_position:3,title:"Chapter 3: Sensor and Environment Integration",description:"Learn to connect simulated sensors to ROS 2 nodes and create complete digital twin systems"},o="Chapter 3: Sensor and Environment Integration",l={},c=[{value:"Overview",id:"overview",level:2},{value:"Connecting Simulated Sensors to ROS 2 Nodes",id:"connecting-simulated-sensors-to-ros-2-nodes",level:2},{value:"Understanding the ROS 2 Bridge Architecture",id:"understanding-the-ros-2-bridge-architecture",level:3},{value:"Setting up ros_gz_bridge",id:"setting-up-ros_gz_bridge",level:3},{value:"Setting up Unity ROS TCP Connector",id:"setting-up-unity-ros-tcp-connector",level:3},{value:"Sensor Data Types and Formats",id:"sensor-data-types-and-formats",level:3},{value:"Handling Data Streams for AI Agents",id:"handling-data-streams-for-ai-agents",level:2},{value:"Data Pipeline Architecture",id:"data-pipeline-architecture",level:3},{value:"Implementing Data Collection",id:"implementing-data-collection",level:3},{value:"Data Preprocessing for AI",id:"data-preprocessing-for-ai",level:3},{value:"Synchronizing Gazebo Physics with Unity Visualization",id:"synchronizing-gazebo-physics-with-unity-visualization",level:2},{value:"Time Synchronization",id:"time-synchronization",level:3},{value:"State Synchronization",id:"state-synchronization",level:3},{value:"Data Consistency Strategies",id:"data-consistency-strategies",level:3},{value:"Preparing Digital Twin for AI Training",id:"preparing-digital-twin-for-ai-training",level:2},{value:"Creating Training Data",id:"creating-training-data",level:3},{value:"Environment Randomization",id:"environment-randomization",level:3},{value:"Performance Optimization for Training",id:"performance-optimization-for-training",level:3},{value:"Complete Integration Example",id:"complete-integration-example",level:2},{value:"Launch File for Full System",id:"launch-file-for-full-system",level:3},{value:"Monitoring and Debugging",id:"monitoring-and-debugging",level:3},{value:"Best Practices and Troubleshooting",id:"best-practices-and-troubleshooting",level:2},{value:"Performance Best Practices",id:"performance-best-practices",level:3},{value:"Common Integration Issues",id:"common-integration-issues",level:3},{value:"Validation Strategies",id:"validation-strategies",level:3},{value:"Summary",id:"summary",level:2},{value:"Navigation",id:"navigation",level:2}];function d(n){const e={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...n.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.header,{children:(0,t.jsx)(e.h1,{id:"chapter-3-sensor-and-environment-integration",children:"Chapter 3: Sensor and Environment Integration"})}),"\n",(0,t.jsx)(e.h2,{id:"overview",children:"Overview"}),"\n",(0,t.jsx)(e.p,{children:"In this final chapter, we'll connect the physics simulation in Gazebo with the visualization in Unity to create a complete digital twin system. You'll learn how to connect simulated sensors to ROS 2 nodes, handle data streams for AI agents, synchronize the two systems, and prepare for AI perception and navigation."}),"\n",(0,t.jsx)(e.h2,{id:"connecting-simulated-sensors-to-ros-2-nodes",children:"Connecting Simulated Sensors to ROS 2 Nodes"}),"\n",(0,t.jsx)(e.h3,{id:"understanding-the-ros-2-bridge-architecture",children:"Understanding the ROS 2 Bridge Architecture"}),"\n",(0,t.jsx)(e.p,{children:"The digital twin system connects three components:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Gazebo Simulation"}),": Provides physics and sensor data"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"ROS 2 Network"}),": Transports data between components"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Unity Visualization"}),": Provides high-fidelity rendering"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"setting-up-ros_gz_bridge",children:"Setting up ros_gz_bridge"}),"\n",(0,t.jsxs)(e.p,{children:["The ",(0,t.jsx)(e.code,{children:"ros_gz_bridge"})," connects ROS 2 topics to Gazebo transport:"]}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-bash",children:"# Install the bridge package\nsudo apt install ros-humble-ros-gz-bridge\n\n# Run the bridge with a configuration file\nros2 run ros_gz_bridge parameter_bridge __params:=path/to/bridge_config.yaml\n"})}),"\n",(0,t.jsxs)(e.p,{children:["Example bridge configuration (",(0,t.jsx)(e.code,{children:"bridge_config.yaml"}),"):"]}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-yaml",children:'/**:\n  ros_gz_bridge:\n    ros__parameters:\n      config_file: "bridge_config.yaml"\n\n# Bridge configuration\n- ros_topic_name: "/cmd_vel"\n  gz_topic_name: "/robot/cmd_vel"\n  ros_type_name: "geometry_msgs/msg/Twist"\n  gz_type_name: "gz.msgs.Twist"\n  direction: ROS_TO_GZ\n\n- ros_topic_name: "/scan"\n  gz_topic_name: "/lidar/scan"\n  ros_type_name: "sensor_msgs/msg/LaserScan"\n  gz_type_name: "gz.msgs.LaserScan"\n  direction: GZ_TO_ROS\n\n- ros_topic_name: "/imu/data"\n  gz_topic_name: "/imu/data"\n  ros_type_name: "sensor_msgs/msg/Imu"\n  gz_type_name: "gz.msgs.IMU"\n  direction: GZ_TO_ROS\n'})}),"\n",(0,t.jsx)(e.h3,{id:"setting-up-unity-ros-tcp-connector",children:"Setting up Unity ROS TCP Connector"}),"\n",(0,t.jsx)(e.p,{children:"Connect Unity to ROS 2 using the TCP connector:"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Configure ROS TCP Connector in Unity"}),":"]}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Add ROS Settings component to your scene"}),"\n",(0,t.jsxs)(e.li,{children:["Set ROS IP to your ROS 2 master (usually ",(0,t.jsx)(e.code,{children:"127.0.0.1"}),")"]}),"\n",(0,t.jsxs)(e.li,{children:["Set port to match your ROS 2 bridge (default ",(0,t.jsx)(e.code,{children:"10000"}),")"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Create publisher/subscriber scripts in Unity"}),":"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-csharp",children:'using Unity.Robotics.ROSTCPConnector;\nusing Unity.Robotics.ROSTCPConnector.MessageTypes.Sensor;\nusing Unity.Robotics.ROSTCPConnector.MessageTypes.Geometry;\n\npublic class SensorBridge : MonoBehaviour\n{\n    ROSConnection ros;\n    string lidarTopic = "unity/lidar_scan";\n    string imuTopic = "unity/imu_data";\n\n    void Start()\n    {\n        ros = ROSConnection.GetOrCreateInstance();\n        ros.RegisterPublisher<LaserScanMsg>(lidarTopic);\n        ros.RegisterPublisher<ImuMsg>(imuTopic);\n    }\n\n    void PublishLidarData()\n    {\n        var lidarMsg = new LaserScanMsg();\n        // Populate lidar message with Unity-generated data\n        ros.Publish(lidarTopic, lidarMsg);\n    }\n\n    void PublishImuData()\n    {\n        var imuMsg = new ImuMsg();\n        // Populate IMU message with Unity-generated data\n        ros.Publish(imuTopic, imuMsg);\n    }\n}\n'})}),"\n",(0,t.jsx)(e.h3,{id:"sensor-data-types-and-formats",children:"Sensor Data Types and Formats"}),"\n",(0,t.jsx)(e.p,{children:"Common sensor data formats for digital twin integration:"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"LaserScan"})," (",(0,t.jsx)(e.code,{children:"sensor_msgs/LaserScan"}),"):"]}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.code,{children:"ranges"}),": Array of distance measurements"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.code,{children:"intensities"}),": Array of intensity values"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.code,{children:"angle_min"}),", ",(0,t.jsx)(e.code,{children:"angle_max"}),": Angular range"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.code,{children:"angle_increment"}),": Angular resolution"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"PointCloud2"})," (",(0,t.jsx)(e.code,{children:"sensor_msgs/PointCloud2"}),"):"]}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.code,{children:"height"}),", ",(0,t.jsx)(e.code,{children:"width"}),": Dimensions of point cloud"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.code,{children:"fields"}),": Description of point fields (x, y, z, etc.)"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.code,{children:"data"}),": Binary data containing points"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Image"})," (",(0,t.jsx)(e.code,{children:"sensor_msgs/Image"}),"):"]}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.code,{children:"height"}),", ",(0,t.jsx)(e.code,{children:"width"}),": Image dimensions"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.code,{children:"encoding"}),": Pixel format (rgb8, bgr8, etc.)"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.code,{children:"data"}),": Raw image data"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Imu"})," (",(0,t.jsx)(e.code,{children:"sensor_msgs/Imu"}),"):"]}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.code,{children:"orientation"}),": Quaternion representing orientation"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.code,{children:"angular_velocity"}),": Angular velocity vector"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.code,{children:"linear_acceleration"}),": Linear acceleration vector"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"handling-data-streams-for-ai-agents",children:"Handling Data Streams for AI Agents"}),"\n",(0,t.jsx)(e.h3,{id:"data-pipeline-architecture",children:"Data Pipeline Architecture"}),"\n",(0,t.jsx)(e.p,{children:"Design an efficient data pipeline for AI agents:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{children:"Sensors \u2192 Data Collection \u2192 Preprocessing \u2192 AI Agent \u2192 Actions \u2192 Actuators\n"})}),"\n",(0,t.jsx)(e.h3,{id:"implementing-data-collection",children:"Implementing Data Collection"}),"\n",(0,t.jsx)(e.p,{children:"Create a data collection node that aggregates sensor data:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import LaserScan, Imu, Image\nfrom std_msgs.msg import String\nimport numpy as np\n\nclass DigitalTwinDataCollector(Node):\n    def __init__(self):\n        super().__init__('digital_twin_data_collector')\n\n        # Subscribe to sensor topics\n        self.lidar_sub = self.create_subscription(\n            LaserScan, '/lidar/scan', self.lidar_callback, 10)\n        self.imu_sub = self.create_subscription(\n            Imu, '/imu/data', self.imu_callback, 10)\n        self.camera_sub = self.create_subscription(\n            Image, '/camera/rgb/image_raw', self.camera_callback, 10)\n\n        # Publisher for processed data\n        self.ai_publisher = self.create_publisher(\n            String, '/ai_agent/input', 10)\n\n        # Data storage\n        self.sensor_data = {\n            'lidar': None,\n            'imu': None,\n            'camera': None\n        }\n\n    def lidar_callback(self, msg):\n        self.sensor_data['lidar'] = np.array(msg.ranges)\n        self.process_and_publish()\n\n    def imu_callback(self, msg):\n        self.sensor_data['imu'] = {\n            'orientation': [msg.orientation.x, msg.orientation.y, msg.orientation.z, msg.orientation.w],\n            'angular_velocity': [msg.angular_velocity.x, msg.angular_velocity.y, msg.angular_velocity.z],\n            'linear_acceleration': [msg.linear_acceleration.x, msg.linear_acceleration.y, msg.linear_acceleration.z]\n        }\n        self.process_and_publish()\n\n    def camera_callback(self, msg):\n        # Convert image message to numpy array\n        # This is a simplified example - actual implementation depends on encoding\n        self.sensor_data['camera'] = np.frombuffer(msg.data, dtype=np.uint8)\n        self.process_and_publish()\n\n    def process_and_publish(self):\n        # Check if all sensor data is available\n        if all(data is not None for data in self.sensor_data.values()):\n            # Process data for AI agent\n            ai_input = self.format_for_ai_agent()\n\n            # Publish to AI agent\n            msg = String()\n            msg.data = ai_input\n            self.ai_publisher.publish(msg)\n\n    def format_for_ai_agent(self):\n        # Format sensor data for AI agent consumption\n        formatted_data = {\n            'lidar_ranges': self.sensor_data['lidar'].tolist(),\n            'imu_orientation': self.sensor_data['imu']['orientation'],\n            'timestamp': self.get_clock().now().nanoseconds\n        }\n        return str(formatted_data)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = DigitalTwinDataCollector()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,t.jsx)(e.h3,{id:"data-preprocessing-for-ai",children:"Data Preprocessing for AI"}),"\n",(0,t.jsx)(e.p,{children:"Preprocess sensor data to make it suitable for AI agents:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'import numpy as np\nfrom scipy import ndimage\n\nclass SensorPreprocessor:\n    def __init__(self):\n        self.lidar_range_min = 0.1\n        self.lidar_range_max = 10.0\n\n    def preprocess_lidar(self, lidar_ranges):\n        """Preprocess LiDAR data for AI agent"""\n        # Replace invalid ranges with max range\n        ranges = np.array(lidar_ranges)\n        ranges[ranges < self.lidar_range_min] = self.lidar_range_max\n        ranges[ranges > self.lidar_range_max] = self.lidar_range_max\n\n        # Normalize to [0, 1]\n        normalized = ranges / self.lidar_range_max\n\n        # Smooth data to reduce noise\n        smoothed = ndimage.gaussian_filter1d(normalized, sigma=1.0)\n\n        return smoothed\n\n    def preprocess_camera(self, image_data, width, height, encoding):\n        """Preprocess camera data for AI agent"""\n        if encoding == \'rgb8\':\n            # Reshape and normalize image\n            img = np.frombuffer(image_data, dtype=np.uint8)\n            img = img.reshape((height, width, 3))\n            normalized = img.astype(np.float32) / 255.0\n        else:\n            # Handle other encodings\n            normalized = image_data\n\n        return normalized\n'})}),"\n",(0,t.jsx)(e.h2,{id:"synchronizing-gazebo-physics-with-unity-visualization",children:"Synchronizing Gazebo Physics with Unity Visualization"}),"\n",(0,t.jsx)(e.h3,{id:"time-synchronization",children:"Time Synchronization"}),"\n",(0,t.jsx)(e.p,{children:"Ensure both systems operate on the same timeline:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom builtin_interfaces.msg import Time\nfrom std_msgs.msg import Header\nimport time\n\nclass TimeSynchronizer(Node):\n    def __init__(self):\n        super().__init__('time_synchronizer')\n\n        # Publisher for synchronized time\n        self.time_pub = self.create_publisher(Time, '/sync_time', 10)\n\n        # Timer for time synchronization\n        self.timer = self.create_timer(0.01, self.publish_time)  # 100Hz\n\n    def publish_time(self):\n        # Get current ROS time\n        current_time = self.get_clock().now().to_msg()\n\n        # Publish to Unity via ROS bridge\n        self.time_pub.publish(current_time)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = TimeSynchronizer()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n"})}),"\n",(0,t.jsx)(e.h3,{id:"state-synchronization",children:"State Synchronization"}),"\n",(0,t.jsx)(e.p,{children:"Keep robot states consistent between systems:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-csharp",children:'// Unity script for receiving state updates\nusing UnityEngine;\nusing Unity.Robotics.ROSTCPConnector;\nusing Unity.Robotics.ROSTCPConnector.MessageTypes.Std;\n\npublic class StateSynchronizer : MonoBehaviour\n{\n    ROSConnection ros;\n    string stateTopic = "robot_states";\n\n    void Start()\n    {\n        ros = ROSConnection.GetOrCreateInstance();\n        ros.Subscribe<Unity.Robotics.ROSTCPConnector.MessageTypes.Std.StringMsg>(stateTopic, OnRobotStateReceived);\n    }\n\n    void OnRobotStateReceived(Unity.Robotics.ROSTCPConnector.MessageTypes.Std.StringMsg stateMsg)\n    {\n        // Parse state data and update Unity objects\n        UpdateRobotPosition(stateMsg.data);\n    }\n\n    void UpdateRobotPosition(string stateData)\n    {\n        // Parse JSON or other format containing position data\n        // Update robot transforms in Unity\n        // Example: {"position": [x, y, z], "rotation": [x, y, z, w]}\n    }\n}\n'})}),"\n",(0,t.jsx)(e.h3,{id:"data-consistency-strategies",children:"Data Consistency Strategies"}),"\n",(0,t.jsx)(e.p,{children:"Maintain consistency between physics and visualization:"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"State Publishing"}),": Publish physics states from Gazebo to Unity"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Interpolation"}),": Smooth transitions between states"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Error Correction"}),": Correct visualization drift over time"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"preparing-digital-twin-for-ai-training",children:"Preparing Digital Twin for AI Training"}),"\n",(0,t.jsx)(e.h3,{id:"creating-training-data",children:"Creating Training Data"}),"\n",(0,t.jsx)(e.p,{children:"Generate diverse training scenarios:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import LaserScan, Image\nimport numpy as np\nimport json\nimport os\n\nclass TrainingDataGenerator(Node):\n    def __init__(self):\n        super().__init__('training_data_generator')\n\n        # Subscriptions\n        self.lidar_sub = self.create_subscription(\n            LaserScan, '/lidar/scan', self.lidar_callback, 10)\n        self.action_sub = self.create_subscription(\n            String, '/ai_agent/action', self.action_callback, 10)\n\n        # Storage for training data\n        self.episode_data = []\n        self.episode_counter = 0\n\n    def lidar_callback(self, msg):\n        # Store sensor data with timestamp\n        data_point = {\n            'timestamp': self.get_clock().now().nanoseconds,\n            'lidar_ranges': list(msg.ranges),\n            'action': None  # Will be filled when action is received\n        }\n        self.episode_data.append(data_point)\n\n    def action_callback(self, msg):\n        # Fill in the action for the most recent data point\n        if self.episode_data:\n            self.episode_data[-1]['action'] = msg.data\n\n    def save_episode(self):\n        # Save current episode data to file\n        filename = f\"episode_{self.episode_counter:04d}.json\"\n        filepath = os.path.join(\"training_data\", filename)\n\n        with open(filepath, 'w') as f:\n            json.dump(self.episode_data, f)\n\n        self.episode_data = []\n        self.episode_counter += 1\n        self.get_logger().info(f\"Saved episode to {filename}\")\n"})}),"\n",(0,t.jsx)(e.h3,{id:"environment-randomization",children:"Environment Randomization"}),"\n",(0,t.jsx)(e.p,{children:"Create diverse training environments:"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Lighting Variation"}),": Randomize lighting conditions"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Object Placement"}),": Randomize obstacle positions"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Sensor Noise"}),": Add realistic sensor noise"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Physics Variation"}),": Slightly vary physical parameters"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"performance-optimization-for-training",children:"Performance Optimization for Training"}),"\n",(0,t.jsx)(e.p,{children:"Optimize for high-throughput training:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-bash",children:"# Run multiple simulation instances\n# Use headless mode for faster simulation\ngzserver --headless-rendering your_world.sdf\n\n# Optimize Gazebo parameters for speed\n# Increase max step size and real-time factor\n# Reduce visual quality if not needed\n"})}),"\n",(0,t.jsx)(e.h2,{id:"complete-integration-example",children:"Complete Integration Example"}),"\n",(0,t.jsx)(e.h3,{id:"launch-file-for-full-system",children:"Launch File for Full System"}),"\n",(0,t.jsx)(e.p,{children:"Create a launch file that starts the complete digital twin:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"# launch/digital_twin_complete.launch.py\nfrom launch import LaunchDescription\nfrom launch.actions import IncludeLaunchDescription, ExecuteProcess, RegisterEventHandler\nfrom launch.launch_description_sources import PythonLaunchDescriptionSource\nfrom launch.substitutions import PathJoinSubstitution\nfrom launch_ros.substitutions import FindPackageShare\nfrom launch.event_handlers import OnProcessExit\nfrom launch.actions import TimerAction\n\ndef generate_launch_description():\n    # Launch Gazebo simulation\n    gzserver = ExecuteProcess(\n        cmd=['gzserver', '--verbose', 'path/to/digital_twin_world.sdf'],\n        output='screen'\n    )\n\n    # Launch Gazebo client (optional, for visualization)\n    gzclient = ExecuteProcess(\n        cmd=['gzclient', '--verbose'],\n        output='screen'\n    )\n\n    # Launch ROS-Gazebo bridge\n    bridge = ExecuteProcess(\n        cmd=['ros2', 'run', 'ros_gz_bridge', 'parameter_bridge',\n             '--ros-args', '--params-file', 'path/to/bridge_config.yaml'],\n        output='screen'\n    )\n\n    # Launch Unity application (assuming it connects via TCP)\n    unity_sim = ExecuteProcess(\n        cmd=['/path/to/unity/build/UnitySimulation'],\n        output='screen'\n    )\n\n    # Launch data collection node\n    data_collector = ExecuteProcess(\n        cmd=['ros2', 'run', 'digital_twin_pkg', 'data_collector'],\n        output='screen'\n    )\n\n    # Launch AI agent\n    ai_agent = ExecuteProcess(\n        cmd=['ros2', 'run', 'ai_agent_pkg', 'ai_agent'],\n        output='screen'\n    )\n\n    return LaunchDescription([\n        gzserver,\n        gzclient,\n        # Add delay before starting bridge to ensure Gazebo is ready\n        TimerAction(\n            period=5.0,\n            actions=[bridge]\n        ),\n        unity_sim,\n        data_collector,\n        ai_agent\n    ])\n"})}),"\n",(0,t.jsx)(e.h3,{id:"monitoring-and-debugging",children:"Monitoring and Debugging"}),"\n",(0,t.jsx)(e.p,{children:"Monitor the integrated system:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom rclpy.qos import QoSProfile\nimport time\n\nclass DigitalTwinMonitor(Node):\n    def __init__(self):\n        super().__init__('digital_twin_monitor')\n\n        # QoS profile for monitoring\n        qos_profile = QoSProfile(depth=10)\n\n        # Track message rates and latencies\n        self.message_stats = {}\n\n    def monitor_topic(self, topic_name, msg_type):\n        \"\"\"Monitor a topic and report statistics\"\"\"\n        def callback(msg):\n            current_time = self.get_clock().now().nanoseconds\n            if topic_name not in self.message_stats:\n                self.message_stats[topic_name] = {\n                    'count': 0,\n                    'last_time': current_time,\n                    'avg_rate': 0\n                }\n\n            stats = self.message_stats[topic_name]\n            time_diff = (current_time - stats['last_time']) / 1e9  # Convert to seconds\n            stats['count'] += 1\n            stats['avg_rate'] = stats['count'] / time_diff if time_diff > 0 else 0\n            stats['last_time'] = current_time\n\n            # Log statistics periodically\n            if stats['count'] % 100 == 0:\n                self.get_logger().info(\n                    f\"{topic_name}: {stats['avg_rate']:.2f} Hz\")\n\n        self.create_subscription(msg_type, topic_name, callback, qos_profile)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    monitor = DigitalTwinMonitor()\n\n    # Monitor key topics\n    monitor.monitor_topic('/lidar/scan', LaserScan)\n    monitor.monitor_topic('/imu/data', Imu)\n    monitor.monitor_topic('/camera/rgb/image_raw', Image)\n\n    try:\n        rclpy.spin(monitor)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        monitor.destroy_node()\n        rclpy.shutdown()\n"})}),"\n",(0,t.jsx)(e.h2,{id:"best-practices-and-troubleshooting",children:"Best Practices and Troubleshooting"}),"\n",(0,t.jsx)(e.h3,{id:"performance-best-practices",children:"Performance Best Practices"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Optimize update rates"}),": Match simulation rates to required fidelity"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Use efficient data structures"}),": Minimize serialization overhead"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Implement data buffering"}),": Handle timing differences gracefully"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Monitor resource usage"}),": Keep track of CPU, memory, and network usage"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"common-integration-issues",children:"Common Integration Issues"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Coordinate system mismatches"}),": Ensure consistent frame conventions"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Timing issues"}),": Handle differences in update rates"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Data type mismatches"}),": Verify message type compatibility"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Network latency"}),": Account for communication delays"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"validation-strategies",children:"Validation Strategies"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Unit testing"}),": Test individual components"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Integration testing"}),": Test system-level behavior"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Performance testing"}),": Verify real-time requirements"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Regression testing"}),": Ensure changes don't break existing functionality"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,t.jsx)(e.p,{children:"In this chapter, you've learned how to:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Connect simulated sensors to ROS 2 nodes using bridges"}),"\n",(0,t.jsx)(e.li,{children:"Handle data streams for AI agents with proper preprocessing"}),"\n",(0,t.jsx)(e.li,{children:"Synchronize Gazebo physics with Unity visualization"}),"\n",(0,t.jsx)(e.li,{children:"Prepare the digital twin system for AI training applications"}),"\n",(0,t.jsx)(e.li,{children:"Implement monitoring and debugging for the integrated system"}),"\n"]}),"\n",(0,t.jsx)(e.p,{children:"You now have a complete digital twin system that combines Gazebo physics simulation, Unity high-fidelity rendering, and ROS 2 communication for AI agent development. This system enables safe testing and experimentation with humanoid robots before real-world deployment."}),"\n",(0,t.jsx)(e.h2,{id:"navigation",children:"Navigation"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:["Previous: ",(0,t.jsx)(e.a,{href:"./02-high-fidelity-rendering-unity",children:"Chapter 2 - High-Fidelity Rendering in Unity"})]}),"\n",(0,t.jsxs)(e.li,{children:["Back to: ",(0,t.jsx)(e.a,{href:"../",children:"Module Overview"})]}),"\n"]})]})}function g(n={}){const{wrapper:e}={...(0,r.R)(),...n.components};return e?(0,t.jsx)(e,{...n,children:(0,t.jsx)(d,{...n})}):d(n)}},8453(n,e,i){i.d(e,{R:()=>a,x:()=>o});var s=i(6540);const t={},r=s.createContext(t);function a(n){const e=s.useContext(r);return s.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function o(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(t):n.components||t:a(n.components),s.createElement(r.Provider,{value:e},n.children)}}}]);